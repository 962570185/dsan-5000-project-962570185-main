[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN-5000: Introduction",
    "section": "",
    "text": "See the following link for more information about the author: about me\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nHIGHLY RECOMMENDED\n\nIt is highly recommended that you build your website using .ipynb files and NOT .qmdfiles\nFunctionally the two formats are basically identical, i.e. they are just Markdown + Code\nHowever there is ONE MAJOR DIFFERENCE, i.e. .ipynb stores the code outputs in the meta-data of the file\n\nThis means you ONLY HAVE TO RUN THE CODE ONCE with .ipynb\n.qmd will run the code every time you build the website, which can be very slow\n\nThere are caching options for .qmd, however, they are “messier” that just using .ipynb\n\nNote: .qmd is fine if there is no code, in which case it is basically just a Markdown file\n\nConverting between the two\n\nYou can switch between the two formats using\nquarto convert clustering.qmd this will output a .ipynb version called clustering.ipynb\nquarto convert eda.ipynb this will output a .qmd version called eda.qmd\n\nYOU CAN RUN R CODE IN VSC WITH .ipynb, see the following link\n\nhttps://saturncloud.io/blog/how-to-use-jupyter-r-kernel-with-visual-studio-code/\n\nIt is possible, but NOT RECOMMENDED, to mix Python and R code in the same file\n\nIMPORTANT ASIDE\n\nA .ipynb file is simply a JSON file with a specialized structural format\nYou can see this by running more eda/eda.ipynb from the command line\nWhich will output the following;\n\n\nTIP FOR MAC USERS\n\ncommand+control+shift+4 is very useful on a mac.\n\nIt takes a screenshot and saves it to the clip-board\n\nThe following VSC extension allows you to paste images from the clip-board with alt+command+v.\n\ntab is your best friend when using the command line, since it does auto-completion\nopen ./path_to_file will open any file or directory from the command line"
  },
  {
    "objectID": "Naive_Bayes_record_data.html",
    "href": "Naive_Bayes_record_data.html",
    "title": "Naïve Bayes (NB) with Labeled Record Data",
    "section": "",
    "text": "Overview of Naive Bayes Classification:\n\n\nNaive Bayes is a probabilistic classification technique based on Bayes’ Theorem with the assumption of independence between predictors (hence “naive”). In simple terms, it predicts the likelihood of an event based on prior knowledge of conditions related to that event.\n\n2. Probabilistic Nature and Bayes’ Theorem Foundation:\n\nNaive Bayes operates on the principle of probability. It calculates the probability of an event in the presence of a particular feature using Bayes’ theorem. Bayes’ theorem finds the probability of an event occurring, given the probability of another related event.\nThe fundamental equation behind this method is: P(A|B) = [P(B|A) * P(A)] / P(B), where:\nP(A|B) is the probability of hypothesis A given data B.\nP(B|A) is the probability of data B given hypothesis A.\nP(A) and P(B) are the probabilities of observing hypothesis A and data B independently of each other.\n\n3. Objectives of Using Naive Bayes:\n\nThe primary objective when using the Naive Bayes classification is to predict the class of the given dataset and to understand the probability that a given data point belongs to a particular category or class.\n\n4. Aims of Naive Bayes Classification:\n\nWith Naive Bayes, the aim is to assign an observation to a class such that the posterior probability is maximized. In other words, it tries to find the class label for a given data point by maximizing the posterior probability.\n\n5. Variants of Naive Bayes:\n\nThere are mainly three types of Naive Bayes models:\nGaussian: It assumes that continuous features follow a normal distribution. It’s used when the data has continuous attributes.\nMultinomial: Suitable for discrete data. It’s commonly used in text classification problems.\nBernoulli: Used for binary/boolean features.\n\n6. When to Use Each Variant:\n\nGaussian is generally used for datasets with continuous data points.\nMultinomial is used for datasets with discrete data points (e.g., word counts for text classification).\nBernoulli is useful when feature vectors are binary (e.g., whether a word occurs in a document or not).\n\n\n# data &lt;- read.csv(\"../.././data./01-modified-data./cleaned_data_r2.csv\", stringsAsFactors = FALSE)\ndata &lt;- read.csv(\"../../data/01-modified-data/cleaned_data_r2.csv\", stringsAsFactors = FALSE)\n\n\nfilter_data &lt;- data[, c(\"Year\", \"Runtime\", \"Rating\", \"Genres\", \"Director\", \"Writers\", \"Cast\")]\n\n\nfilter_data$Genres &lt;- sub(\"\\\\|.*\", \"\", filter_data$Genres)\nfilter_data$Cast &lt;- sub(\"\\\\|.*\", \"\", filter_data$Cast)\n\n\nfilter_data &lt;- filter_data[filter_data$Runtime != 0, ]\nstr(filter_data)\n\n'data.frame':   3747 obs. of  7 variables:\n $ Year    : int  2017 2005 2001 2017 2000 2018 2007 2017 2011 2018 ...\n $ Runtime : int  66 75 65 99 79 95 91 91 107 108 ...\n $ Rating  : num  7.4 7.9 6.8 7.6 6.4 6.6 7.2 8.1 7.4 6.4 ...\n $ Genres  : chr  \"Uncategorized\" \"Documentary\" \"Adventure\" \"Drama\" ...\n $ Director: chr  \"Bobcat Goldthwait\" \"Greg Whiteley\" \"Tony Craig\" \"Vincent Grashaw\" ...\n $ Writers : chr  \"Patton Oswalt\" \"Arthur Kane\" \"Thomas Hart\" \"Brett Haley\" ...\n $ Cast    : chr  \"Patton Oswalt\" \"Sylvain Sylvain\" \"Carlos Alazraqui\" \"Arman Darbo\" ...\n\n\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nset.seed(42)\n\ninitialSplit &lt;- createDataPartition(filter_data$Rating, p = 0.8, list = FALSE)\ntrainValid_data &lt;- filter_data[initialSplit, ]\ntest_data &lt;- filter_data[-initialSplit, ]\n\ntrainIndex &lt;- createDataPartition(trainValid_data$Rating, p = 0.75, list = FALSE)\ntrain_data &lt;- trainValid_data[trainIndex, ]\nvalid_data &lt;- trainValid_data[-trainIndex, ]\n\nExplain how and why is it necessary to split a dataset into training and test sets.\nDatasets are often split into training and test sets for machine learning and statistical modeling needs. Here are the reasons why:\nAvoiding Overfitting: By testing the model on separate datasets, we can avoid overfitting the model to the training data (also known as overfitting). If the model performs well on the training set but not on the test set, this usually means that the model is overfitting.\nEvaluating Performance: The test set provides us with a tool to evaluate the performance of the model on new, unseen data. This provides us with an objective measure of the model’s ability to generalize.\nModel Selection: By comparing the performance of different models or parameter settings on the test set, we can select the best model or parameter combination.\n\nlibrary(e1071)\nlibrary(caret)\n\nmodel_NB &lt;- naiveBayes(Rating ~ ., data=train_data)\n\n\npredictions &lt;- predict(model_NB, test_data)\n\nall_levels &lt;- unique(c(levels(predictions), levels(test_data$Rating)))\n\npredictions &lt;- factor(predictions, levels = all_levels)\ntest_data$Rating &lt;- factor(test_data$Rating, levels = all_levels)\n\ncm &lt;- confusionMatrix(predictions, test_data$Rating)\nprint(cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 1.7 2.3 2.4 2.5 2.7 2.8 2.9 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4\n       1.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       2.3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       2.4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       2.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       2.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       2.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       2.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.6   0   0   0   0   1   0   0   0   1   0   0   1   0   0   0   0 1\n       3.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5     0   0   0   1   0   0   0   0   0   0   0   0   1   0   0   0 0\n       5.1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.4   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0 1\n       5.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.3   0   0   1   0   0   0   0   0   0   0   0   1   0   0   0   0 0\n       6.4   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0 0\n       6.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       9     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       9.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n          Reference\nPrediction 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5 5.1 5.2 5.3 5.4 5.5 5.6 5.7\n       1.7   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       2.3   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       2.4   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       2.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       2.7   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       2.8   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       2.9   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.1   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.2   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.3   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.4   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.6   0   0   0   0   0   0   0   0   0 1   2   1   0   0   0   1   0\n       3.7   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.8   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.9   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4     0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.1   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.2   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.3   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.4   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.6   0   0   0   1   0   0   1   0   1 0   0   0   0   0   0   0   0\n       4.7   0   0   1   0   0   0   0   0   0 1   0   0   0   0   0   0   0\n       4.8   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.9   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       5     1   0   0   0   0   1   1   0   0 2   1   0   3   0   1   0   0\n       5.1   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       5.2   0   0   0   0   0   0   0   0   0 1   0   0   0   1   0   1   1\n       5.3   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   1   0\n       5.4   0   0   1   0   0   0   1   0   0 0   1   0   0   1   1   2   1\n       5.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       5.6   0   0   0   0   0   0   0   0   0 0   0   2   1   1   0   0   1\n       5.7   0   0   0   0   0   1   0   0   0 0   1   0   0   0   0   1   0\n       5.8   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   1   0\n       5.9   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       6     0   0   0   0   1   0   0   0   0 0   0   0   1   0   0   1   0\n       6.1   0   0   0   0   0   0   0   0   0 0   1   0   0   0   0   0   1\n       6.2   0   0   0   1   1   0   0   1   0 2   1   1   0   1   1   0   0\n       6.3   0   0   0   1   0   0   0   0   0 3   0   2   1   1   0   0   4\n       6.4   1   1   3   0   1   1   1   0   0 3   0   1   0   2   2   1   4\n       6.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   1   0   2\n       6.6   0   0   0   0   0   0   0   0   0 0   0   1   2   0   0   0   0\n       6.7   0   0   0   0   0   0   0   0   0 0   0   0   0   0   1   1   1\n       6.8   0   0   0   0   0   0   0   0   1 0   0   0   0   0   0   0   0\n       6.9   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       7     0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       7.1   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       7.2   0   0   0   0   0   0   0   0   0 0   0   0   1   0   0   1   0\n       7.3   0   0   0   0   0   0   0   0   0 0   0   0   1   0   0   0   0\n       7.4   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   1   0\n       7.5   1   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       7.6   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       7.7   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       7.8   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   1\n       7.9   0   0   0   0   0   0   0   0   0 0   0   0   1   0   0   0   0\n       8     0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.1   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.2   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.3   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.4   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.6   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.7   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.8   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.9   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       9     0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       9.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n          Reference\nPrediction 5.8 5.9 6 6.1 6.2 6.3 6.4 6.5 6.6 6.7 6.8 6.9 7 7.1 7.2 7.3 7.4 7.5\n       1.7   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       2.3   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       2.4   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       2.5   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       2.7   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       2.8   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       2.9   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.1   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.2   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.3   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.4   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.5   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.6   1   0 0   0   0   0   0   2   0   0   2   0 1   1   1   1   1   1\n       3.7   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.8   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.9   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4     0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4.1   0   0 0   0   0   0   0   1   2   0   0   0 1   0   1   0   0   0\n       4.2   0   0 0   0   0   0   0   0   0   0   0   1 0   0   0   0   0   0\n       4.3   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4.4   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4.5   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4.6   0   0 0   1   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4.7   0   0 0   1   1   0   0   0   0   0   0   0 2   0   0   0   0   0\n       4.8   0   0 0   1   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4.9   0   0 0   0   1   0   0   0   0   0   0   0 0   0   0   0   0   0\n       5     3   0 3   1   1   3   0   0   1   1   0   0 0   0   1   0   0   0\n       5.1   0   0 0   1   0   1   0   0   1   0   0   0 0   0   0   0   1   1\n       5.2   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   1\n       5.3   1   0 0   0   0   1   0   0   0   0   0   0 0   0   0   0   0   0\n       5.4   1   3 1   2   4   2   2   2   5   0   0   0 0   1   0   1   0   0\n       5.5   0   1 0   1   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       5.6   0   0 0   0   0   0   0   2   0   0   1   0 1   0   0   0   1   0\n       5.7   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       5.8   0   0 1   0   4   2   1   0   0   0   1   1 0   0   1   0   2   1\n       5.9   0   0 0   0   0   1   1   1   0   1   1   1 0   0   0   0   0   0\n       6     2   0 0   0   0   1   1   1   0   0   4   1 0   1   0   1   0   0\n       6.1   0   0 0   0   1   3   1   1   4   2   3   2 4   1   0   2   0   0\n       6.2   0   2 4   9   7   4   5   1   7   5   2   1 1   6   1   3   2   2\n       6.3   2   3 1   1   3   2   5   4   3   3   2   3 1   2   3   0   2   0\n       6.4   0   2 3   2   8   4   5   2   5   3   2   2 2   6   4   0   2   0\n       6.5   0   0 1   7   7   4   3   3   2   4   3   2 2   5   2   1   3   0\n       6.6   1   2 1   2   4   0   1   0   0   1   0   2 3   6   0   1   0   1\n       6.7   0   1 0   0   2   0   1   2   6   3   1   2 1   0   3   2   1   1\n       6.8   2   1 0   1   3   0   2   1   2   3   2   0 2   1   0   2   2   1\n       6.9   0   0 0   0   0   0   0   0   1   1   0   0 1   1   1   1   0   0\n       7     1   0 0   0   1   0   1   1   0   1   0   2 0   1   1   1   1   1\n       7.1   0   0 1   1   1   0   0   4   0   0   2   1 0   1   1   2   1   2\n       7.2   1   1 0   1   1   0   3   3   1   1   0   2 1   0   1   1   3   1\n       7.3   0   0 1   0   1   2   0   0   0   2   1   1 1   3   1   1   0   1\n       7.4   0   0 0   0   0   0   0   0   0   0   0   0 0   1   2   0   1   0\n       7.5   0   0 0   1   0   0   0   1   0   1   1   0 0   0   0   1   1   3\n       7.6   0   1 0   1   1   0   0   0   1   0   1   0 0   0   1   0   1   1\n       7.7   0   0 0   0   0   1   0   1   0   0   1   1 0   1   1   2   0   0\n       7.8   0   0 0   0   0   1   0   0   1   0   0   2 0   1   1   0   1   0\n       7.9   1   0 0   0   0   0   0   2   0   0   0   0 2   0   0   0   0   0\n       8     0   0 0   0   0   0   0   1   1   0   0   1 0   1   0   0   1   0\n       8.1   0   0 0   0   0   0   1   1   0   0   1   0 2   1   0   0   0   0\n       8.2   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       8.3   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       8.4   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       8.5   0   0 1   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       8.6   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       8.7   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       8.8   0   0 0   0   0   0   0   0   0   0   0   0 0   1   0   0   0   0\n       8.9   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       9     0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       9.5   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n          Reference\nPrediction 7.6 7.7 7.8 7.9 8 8.1 8.2 8.3 8.4 8.5 8.6 8.7 8.8 8.9 9 9.5\n       1.7   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       2.3   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       2.4   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       2.5   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       2.7   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       2.8   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       2.9   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.1   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.2   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.3   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.4   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.5   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.6   0   1   0   1 1   0   0   0   0   0   0   0   0   0 0   0\n       3.7   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.8   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.9   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4     0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.1   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.2   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.3   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.4   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.5   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.6   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.7   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.8   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.9   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5     0   0   0   1 0   1   0   0   0   0   0   0   0   0 0   0\n       5.1   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.2   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.3   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.4   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.5   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.6   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.7   0   1   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.8   0   0   1   1 0   0   0   0   0   0   0   0   0   0 0   0\n       5.9   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       6     1   0   0   1 0   0   0   0   0   0   0   0   0   0 0   0\n       6.1   0   0   0   0 0   0   0   1   0   0   0   0   0   0 0   0\n       6.2   2   1   2   0 1   0   0   0   0   0   0   0   0   0 0   0\n       6.3   0   0   0   0 0   0   1   0   0   0   0   0   0   0 0   0\n       6.4   1   2   1   0 0   2   0   0   0   0   0   0   0   0 0   0\n       6.5   0   1   1   2 1   0   0   0   0   0   0   0   0   0 0   0\n       6.6   0   1   0   0 0   0   1   0   0   0   0   0   0   0 0   0\n       6.7   0   0   1   0 0   1   0   0   0   0   0   0   0   0 0   0\n       6.8   1   0   1   0 0   0   0   0   0   0   0   0   0   0 0   0\n       6.9   1   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       7     0   0   1   0 2   0   0   0   0   0   0   0   0   0 0   0\n       7.1   0   3   0   0 1   1   0   0   0   0   0   0   0   0 0   0\n       7.2   1   1   0   1 0   0   0   0   0   0   1   0   0   0 0   0\n       7.3   2   0   2   0 1   0   1   1   0   0   0   0   0   0 0   0\n       7.4   2   0   1   1 1   0   0   0   1   0   0   0   0   0 0   0\n       7.5   1   0   0   1 0   1   0   0   0   0   0   0   0   0 0   0\n       7.6   0   1   2   1 1   1   1   0   0   0   0   0   0   0 0   0\n       7.7   1   1   0   0 1   3   0   0   0   0   0   0   0   0 0   0\n       7.8   1   0   1   0 1   0   0   0   0   0   0   0   0   0 0   0\n       7.9   0   0   1   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8     0   0   2   0 1   0   0   0   0   0   0   0   0   0 0   0\n       8.1   0   0   1   2 1   0   0   1   1   0   0   0   0   0 0   0\n       8.2   0   0   1   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8.3   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8.4   0   0   0   0 0   0   0   0   0   1   0   0   0   0 0   0\n       8.5   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8.6   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8.7   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8.8   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8.9   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       9     0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       9.5   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n\nOverall Statistics\n                                          \n               Accuracy : 0.0469          \n                 95% CI : (0.0329, 0.0646)\n    No Information Rate : 0.0684          \n    P-Value [Acc &gt; NIR] : 0.994           \n                                          \n                  Kappa : 0.0106          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: 1.7 Class: 2.3 Class: 2.4 Class: 2.5 Class: 2.7\nSensitivity                  NA         NA    0.00000    0.00000    0.00000\nSpecificity                   1          1    1.00000    1.00000    1.00000\nPos Pred Value               NA         NA        NaN        NaN        NaN\nNeg Pred Value               NA         NA    0.99866    0.99866    0.99866\nPrevalence                    0          0    0.00134    0.00134    0.00134\nDetection Rate                0          0    0.00000    0.00000    0.00000\nDetection Prevalence          0          0    0.00000    0.00000    0.00000\nBalanced Accuracy            NA         NA    0.50000    0.50000    0.50000\n                     Class: 2.8 Class: 2.9 Class: 3.1 Class: 3.2 Class: 3.3\nSensitivity             0.00000         NA         NA    0.00000         NA\nSpecificity             1.00000          1          1    1.00000          1\nPos Pred Value              NaN         NA         NA        NaN         NA\nNeg Pred Value          0.99866         NA         NA    0.99866         NA\nPrevalence              0.00134          0          0    0.00134          0\nDetection Rate          0.00000          0          0    0.00000          0\nDetection Prevalence    0.00000          0          0    0.00000          0\nBalanced Accuracy       0.50000         NA         NA    0.50000         NA\n                     Class: 3.4 Class: 3.5 Class: 3.6 Class: 3.7 Class: 3.8\nSensitivity             0.00000   0.000000    0.00000         NA         NA\nSpecificity             1.00000   1.000000    0.96913          1          1\nPos Pred Value              NaN        NaN    0.00000         NA         NA\nNeg Pred Value          0.99866   0.997319    0.99862         NA         NA\nPrevalence              0.00134   0.002681    0.00134          0          0\nDetection Rate          0.00000   0.000000    0.00000          0          0\nDetection Prevalence    0.00000   0.000000    0.03083          0          0\nBalanced Accuracy       0.50000   0.500000    0.48456         NA         NA\n                     Class: 3.9 Class: 4 Class: 4.1 Class: 4.2 Class: 4.3\nSensitivity                  NA 0.000000   0.000000    0.00000   0.000000\nSpecificity                   1 1.000000   0.993271    0.99866   1.000000\nPos Pred Value               NA      NaN   0.000000    0.00000        NaN\nNeg Pred Value               NA 0.997319   0.995951    0.99866   0.993298\nPrevalence                    0 0.002681   0.004021    0.00134   0.006702\nDetection Rate                0 0.000000   0.000000    0.00000   0.000000\nDetection Prevalence          0 0.000000   0.006702    0.00134   0.000000\nBalanced Accuracy            NA 0.500000   0.496635    0.49933   0.500000\n                     Class: 4.4 Class: 4.5 Class: 4.6 Class: 4.7 Class: 4.8\nSensitivity            0.000000   0.000000   0.000000   0.000000    0.00000\nSpecificity            1.000000   1.000000   0.994616   0.991914    0.99866\nPos Pred Value              NaN        NaN   0.000000   0.000000    0.00000\nNeg Pred Value         0.995979   0.995979   0.995957   0.994595    0.99866\nPrevalence             0.004021   0.004021   0.004021   0.005362    0.00134\nDetection Rate         0.000000   0.000000   0.000000   0.000000    0.00000\nDetection Prevalence   0.000000   0.000000   0.005362   0.008043    0.00134\nBalanced Accuracy      0.500000   0.500000   0.497308   0.495957    0.49933\n                     Class: 4.9 Class: 5 Class: 5.1 Class: 5.2 Class: 5.3\nSensitivity            0.000000 0.153846   0.000000   0.000000   0.000000\nSpecificity            0.998656 0.964529   0.993234   0.993225   0.995918\nPos Pred Value         0.000000 0.071429   0.000000   0.000000   0.000000\nNeg Pred Value         0.997315 0.984680   0.990553   0.989204   0.985195\nPrevalence             0.002681 0.017426   0.009383   0.010724   0.014745\nDetection Rate         0.000000 0.002681   0.000000   0.000000   0.000000\nDetection Prevalence   0.001340 0.037534   0.006702   0.006702   0.004021\nBalanced Accuracy      0.499328 0.559188   0.496617   0.496612   0.497959\n                     Class: 5.4 Class: 5.5 Class: 5.6 Class: 5.7 Class: 5.8\nSensitivity            0.142857   0.000000    0.00000   0.000000    0.00000\nSpecificity            0.955345   0.997294    0.98638   0.994521    0.97671\nPos Pred Value         0.029412   0.000000    0.00000   0.000000    0.00000\nNeg Pred Value         0.991573   0.990591    0.98370   0.978437    0.97805\nPrevalence             0.009383   0.009383    0.01609   0.021448    0.02145\nDetection Rate         0.001340   0.000000    0.00000   0.000000    0.00000\nDetection Prevalence   0.045576   0.002681    0.01340   0.005362    0.02279\nBalanced Accuracy      0.549101   0.498647    0.49319   0.497260    0.48836\n                     Class: 5.9 Class: 6 Class: 6.1 Class: 6.2 Class: 6.3\nSensitivity            0.000000  0.00000    0.00000   0.137255   0.062500\nSpecificity            0.991770  0.97665    0.96208   0.899281   0.925770\nPos Pred Value         0.000000  0.00000    0.00000   0.090909   0.036364\nNeg Pred Value         0.977027  0.97531    0.95271   0.934230   0.956585\nPrevalence             0.022788  0.02413    0.04558   0.068365   0.042895\nDetection Rate         0.000000  0.00000    0.00000   0.009383   0.002681\nDetection Prevalence   0.008043  0.02279    0.03619   0.103217   0.073727\nBalanced Accuracy      0.495885  0.48832    0.48104   0.518268   0.494135\n                     Class: 6.4 Class: 6.5 Class: 6.6 Class: 6.7 Class: 6.8\nSensitivity            0.151515   0.081081    0.00000   0.093750   0.064516\nSpecificity            0.894811   0.923836    0.95733   0.960784   0.963636\nPos Pred Value         0.062500   0.052632    0.00000   0.096774   0.071429\nNeg Pred Value         0.957958   0.950653    0.93994   0.959441   0.959610\nPrevalence             0.044236   0.049598    0.05764   0.042895   0.041555\nDetection Rate         0.006702   0.004021    0.00000   0.004021   0.002681\nDetection Prevalence   0.107239   0.076408    0.04021   0.041555   0.037534\nBalanced Accuracy      0.523163   0.502459    0.47866   0.527267   0.514076\n                     Class: 6.9 Class: 7 Class: 7.1 Class: 7.2 Class: 7.3\nSensitivity            0.000000  0.00000    0.02381    0.03704    0.04348\nSpecificity            0.990251  0.97911    0.97017    0.96384    0.96957\nPos Pred Value         0.000000  0.00000    0.04545    0.03704    0.04348\nNeg Pred Value         0.962111  0.96170    0.94337    0.96384    0.96957\nPrevalence             0.037534  0.03753    0.05630    0.03619    0.03083\nDetection Rate         0.000000  0.00000    0.00134    0.00134    0.00134\nDetection Prevalence   0.009383  0.02011    0.02949    0.03619    0.03083\nBalanced Accuracy      0.495125  0.48955    0.49699    0.50044    0.50652\n                     Class: 7.4 Class: 7.5 Class: 7.6 Class: 7.7 Class: 7.8\nSensitivity             0.03704   0.166667    0.00000    0.07692    0.05263\nSpecificity             0.98609   0.986264    0.97951    0.98226    0.98624\nPos Pred Value          0.09091   0.230769    0.00000    0.07143    0.09091\nNeg Pred Value          0.96463   0.979536    0.98085    0.98361    0.97551\nPrevalence              0.03619   0.024129    0.01877    0.01743    0.02547\nDetection Rate          0.00134   0.004021    0.00000    0.00134    0.00134\nDetection Prevalence    0.01475   0.017426    0.02011    0.01877    0.01475\nBalanced Accuracy       0.51156   0.576465    0.48975    0.52959    0.51944\n                     Class: 7.9 Class: 8 Class: 8.1 Class: 8.2 Class: 8.3\nSensitivity            0.000000  0.07692    0.00000   0.000000   0.000000\nSpecificity            0.990463  0.99045    0.98370   0.998652   1.000000\nPos Pred Value         0.000000  0.12500    0.00000   0.000000        NaN\nNeg Pred Value         0.983762  0.98374    0.98638   0.994631   0.995979\nPrevalence             0.016086  0.01743    0.01340   0.005362   0.004021\nDetection Rate         0.000000  0.00134    0.00000   0.000000   0.000000\nDetection Prevalence   0.009383  0.01072    0.01609   0.001340   0.000000\nBalanced Accuracy      0.495232  0.53369    0.49185   0.499326   0.500000\n                     Class: 8.4 Class: 8.5 Class: 8.6 Class: 8.7 Class: 8.8\nSensitivity            0.000000    0.00000    0.00000         NA         NA\nSpecificity            0.998656    0.99866    1.00000          1    0.99866\nPos Pred Value         0.000000    0.00000        NaN         NA         NA\nNeg Pred Value         0.997315    0.99866    0.99866         NA         NA\nPrevalence             0.002681    0.00134    0.00134          0    0.00000\nDetection Rate         0.000000    0.00000    0.00000          0    0.00000\nDetection Prevalence   0.001340    0.00134    0.00000          0    0.00134\nBalanced Accuracy      0.499328    0.49933    0.50000         NA         NA\n                     Class: 8.9 Class: 9 Class: 9.5\nSensitivity                  NA       NA         NA\nSpecificity                   1        1          1\nPos Pred Value               NA       NA         NA\nNeg Pred Value               NA       NA         NA\nPrevalence                    0        0          0\nDetection Rate                0        0          0\nDetection Prevalence          0        0          0\nBalanced Accuracy            NA       NA         NA\n\n\n\naccuracy &lt;- cm$overall['Accuracy']\nprecision &lt;- cm$byClass['Precision']\nrecall &lt;- cm$byClass['Recall']\nF1 &lt;- 2 * (precision * recall) / (precision + recall)\naccuracy\n\n  Accuracy \n0.04691689 \n\nprecision\n\n[1] NA\n\nrecall\n\n[1] NA\n\nF1\n\n[1] NA\n\n\n\nlibrary(ggplot2)\nlibrary(reshape2) \n\nnumeric_data &lt;- filter_data[, sapply(filter_data, is.numeric)]\ncor_matrix &lt;- cor(numeric_data, use = \"complete.obs\")\n\nmelted_cormat &lt;- melt(cor_matrix)\nggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + \n  geom_tile() +\n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", \n                       midpoint=0, limit=c(-1,1), space=\"Lab\", \n                       name=\"Correlation\") +\n  theme_minimal() + \n  labs(title = \"Correlation Heatmap\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                    size = 10, hjust = 1))\n\n\n\n\n\nggplot(filter_data, aes(x=Runtime, y=Rating)) +\n  geom_point(alpha=0.7) +\n  labs(title=\"Scatter plot of Runtime vs. Rating\", x=\"Runtime\", y=\"Rating\")\n\n\n\n\n\nfilter_data$Year &lt;- (filter_data$Year - min(filter_data$Year)) / (max(filter_data$Year) - min(filter_data$Year))\n\nggplot(filter_data, aes(x=Year, y=Rating)) +\n  geom_point(alpha=0.7) +\n  labs(title=\"Scatter plot of Year vs. Rating\", x=\"Year\", y=\"Rating\")\n\n\n\n\n\nlibrary(ggplot2)\n\ncomparison &lt;- as.data.frame(cbind(Actual=test_data$Rating, Predicted=predictions))\nggplot(comparison, aes(x=Actual, y=Predicted)) + geom_point(aes(color=Actual)) + \n  ggtitle('Actual vs Predicted') + theme_minimal()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nDiscuss the concepts of overfitting and under-fitting and whether your model is doing it.\nI’ve added as many relevant features as I can, and the model has close to 4,000 records, but the accuracy is very low, suggesting that the correlation between these features is very low. I will try to find more relevant datasets to repeat this question in subsequent assignments.\nDiscuss the model’s performance in terms of accuracy and other relevant metrics.\nThe accuracy is too low causing several other metrics of my model to show NA, I will try to find a more compliant dataset in subsequent assignments\nConclusion\nBy analyzing the movie data using a plain Bayesian model, we obtained some interesting findings. First, the accuracy of the model may not be as high as we expect, suggesting that our data features may not fully capture all the important information that determines movie ratings. In addition, the heatmap of the confusion matrix shows that the model predicts certain ratings more accurately than others. The distribution plots of the features further reveal possible trends between specific ratings and certain features, which provides insight into the relationship between movie ratings and these features. While the performance of the model could be improved, this analysis provides us with a benchmark that could be considered in the future to further optimize model performance using more complex models or more features. Overall, this study provides an initial understanding of the relationship between movie ratings and their features and provides a solid foundation for future in-depth research."
  },
  {
    "objectID": "Naive_Bayes_record_data.html#section",
    "href": "Naive_Bayes_record_data.html#section",
    "title": "Naïve Bayes (NB) with Labeled Record Data",
    "section": "",
    "text": "Overview of Naive Bayes Classification:\n\n\nNaive Bayes is a probabilistic classification technique based on Bayes’ Theorem with the assumption of independence between predictors (hence “naive”). In simple terms, it predicts the likelihood of an event based on prior knowledge of conditions related to that event.\n\n2. Probabilistic Nature and Bayes’ Theorem Foundation:\n\nNaive Bayes operates on the principle of probability. It calculates the probability of an event in the presence of a particular feature using Bayes’ theorem. Bayes’ theorem finds the probability of an event occurring, given the probability of another related event.\nThe fundamental equation behind this method is: P(A|B) = [P(B|A) * P(A)] / P(B), where:\nP(A|B) is the probability of hypothesis A given data B.\nP(B|A) is the probability of data B given hypothesis A.\nP(A) and P(B) are the probabilities of observing hypothesis A and data B independently of each other.\n\n3. Objectives of Using Naive Bayes:\n\nThe primary objective when using the Naive Bayes classification is to predict the class of the given dataset and to understand the probability that a given data point belongs to a particular category or class.\n\n4. Aims of Naive Bayes Classification:\n\nWith Naive Bayes, the aim is to assign an observation to a class such that the posterior probability is maximized. In other words, it tries to find the class label for a given data point by maximizing the posterior probability.\n\n5. Variants of Naive Bayes:\n\nThere are mainly three types of Naive Bayes models:\nGaussian: It assumes that continuous features follow a normal distribution. It’s used when the data has continuous attributes.\nMultinomial: Suitable for discrete data. It’s commonly used in text classification problems.\nBernoulli: Used for binary/boolean features.\n\n6. When to Use Each Variant:\n\nGaussian is generally used for datasets with continuous data points.\nMultinomial is used for datasets with discrete data points (e.g., word counts for text classification).\nBernoulli is useful when feature vectors are binary (e.g., whether a word occurs in a document or not).\n\n\n# data &lt;- read.csv(\"../.././data./01-modified-data./cleaned_data_r2.csv\", stringsAsFactors = FALSE)\ndata &lt;- read.csv(\"../../data/01-modified-data/cleaned_data_r2.csv\", stringsAsFactors = FALSE)\n\n\nfilter_data &lt;- data[, c(\"Year\", \"Runtime\", \"Rating\", \"Genres\", \"Director\", \"Writers\", \"Cast\")]\n\n\nfilter_data$Genres &lt;- sub(\"\\\\|.*\", \"\", filter_data$Genres)\nfilter_data$Cast &lt;- sub(\"\\\\|.*\", \"\", filter_data$Cast)\n\n\nfilter_data &lt;- filter_data[filter_data$Runtime != 0, ]\nstr(filter_data)\n\n'data.frame':   3747 obs. of  7 variables:\n $ Year    : int  2017 2005 2001 2017 2000 2018 2007 2017 2011 2018 ...\n $ Runtime : int  66 75 65 99 79 95 91 91 107 108 ...\n $ Rating  : num  7.4 7.9 6.8 7.6 6.4 6.6 7.2 8.1 7.4 6.4 ...\n $ Genres  : chr  \"Uncategorized\" \"Documentary\" \"Adventure\" \"Drama\" ...\n $ Director: chr  \"Bobcat Goldthwait\" \"Greg Whiteley\" \"Tony Craig\" \"Vincent Grashaw\" ...\n $ Writers : chr  \"Patton Oswalt\" \"Arthur Kane\" \"Thomas Hart\" \"Brett Haley\" ...\n $ Cast    : chr  \"Patton Oswalt\" \"Sylvain Sylvain\" \"Carlos Alazraqui\" \"Arman Darbo\" ...\n\n\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nset.seed(42)\n\ninitialSplit &lt;- createDataPartition(filter_data$Rating, p = 0.8, list = FALSE)\ntrainValid_data &lt;- filter_data[initialSplit, ]\ntest_data &lt;- filter_data[-initialSplit, ]\n\ntrainIndex &lt;- createDataPartition(trainValid_data$Rating, p = 0.75, list = FALSE)\ntrain_data &lt;- trainValid_data[trainIndex, ]\nvalid_data &lt;- trainValid_data[-trainIndex, ]\n\nExplain how and why is it necessary to split a dataset into training and test sets.\nDatasets are often split into training and test sets for machine learning and statistical modeling needs. Here are the reasons why:\nAvoiding Overfitting: By testing the model on separate datasets, we can avoid overfitting the model to the training data (also known as overfitting). If the model performs well on the training set but not on the test set, this usually means that the model is overfitting.\nEvaluating Performance: The test set provides us with a tool to evaluate the performance of the model on new, unseen data. This provides us with an objective measure of the model’s ability to generalize.\nModel Selection: By comparing the performance of different models or parameter settings on the test set, we can select the best model or parameter combination.\n\nlibrary(e1071)\nlibrary(caret)\n\nmodel_NB &lt;- naiveBayes(Rating ~ ., data=train_data)\n\n\npredictions &lt;- predict(model_NB, test_data)\n\nall_levels &lt;- unique(c(levels(predictions), levels(test_data$Rating)))\n\npredictions &lt;- factor(predictions, levels = all_levels)\ntest_data$Rating &lt;- factor(test_data$Rating, levels = all_levels)\n\ncm &lt;- confusionMatrix(predictions, test_data$Rating)\nprint(cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 1.7 2.3 2.4 2.5 2.7 2.8 2.9 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4\n       1.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       2.3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       2.4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       2.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       2.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       2.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       2.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.6   0   0   0   0   1   0   0   0   1   0   0   1   0   0   0   0 1\n       3.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       3.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       4.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5     0   0   0   1   0   0   0   0   0   0   0   0   1   0   0   0 0\n       5.1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.4   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0 1\n       5.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       5.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.3   0   0   1   0   0   0   0   0   0   0   0   1   0   0   0   0 0\n       6.4   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0 0\n       6.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       6.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       7.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       8.9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       9     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n       9.5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 0\n          Reference\nPrediction 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5 5.1 5.2 5.3 5.4 5.5 5.6 5.7\n       1.7   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       2.3   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       2.4   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       2.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       2.7   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       2.8   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       2.9   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.1   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.2   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.3   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.4   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.6   0   0   0   0   0   0   0   0   0 1   2   1   0   0   0   1   0\n       3.7   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.8   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       3.9   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4     0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.1   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.2   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.3   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.4   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.6   0   0   0   1   0   0   1   0   1 0   0   0   0   0   0   0   0\n       4.7   0   0   1   0   0   0   0   0   0 1   0   0   0   0   0   0   0\n       4.8   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       4.9   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       5     1   0   0   0   0   1   1   0   0 2   1   0   3   0   1   0   0\n       5.1   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       5.2   0   0   0   0   0   0   0   0   0 1   0   0   0   1   0   1   1\n       5.3   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   1   0\n       5.4   0   0   1   0   0   0   1   0   0 0   1   0   0   1   1   2   1\n       5.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       5.6   0   0   0   0   0   0   0   0   0 0   0   2   1   1   0   0   1\n       5.7   0   0   0   0   0   1   0   0   0 0   1   0   0   0   0   1   0\n       5.8   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   1   0\n       5.9   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       6     0   0   0   0   1   0   0   0   0 0   0   0   1   0   0   1   0\n       6.1   0   0   0   0   0   0   0   0   0 0   1   0   0   0   0   0   1\n       6.2   0   0   0   1   1   0   0   1   0 2   1   1   0   1   1   0   0\n       6.3   0   0   0   1   0   0   0   0   0 3   0   2   1   1   0   0   4\n       6.4   1   1   3   0   1   1   1   0   0 3   0   1   0   2   2   1   4\n       6.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   1   0   2\n       6.6   0   0   0   0   0   0   0   0   0 0   0   1   2   0   0   0   0\n       6.7   0   0   0   0   0   0   0   0   0 0   0   0   0   0   1   1   1\n       6.8   0   0   0   0   0   0   0   0   1 0   0   0   0   0   0   0   0\n       6.9   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       7     0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       7.1   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       7.2   0   0   0   0   0   0   0   0   0 0   0   0   1   0   0   1   0\n       7.3   0   0   0   0   0   0   0   0   0 0   0   0   1   0   0   0   0\n       7.4   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   1   0\n       7.5   1   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       7.6   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       7.7   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       7.8   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   1\n       7.9   0   0   0   0   0   0   0   0   0 0   0   0   1   0   0   0   0\n       8     0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.1   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.2   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.3   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.4   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.6   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.7   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.8   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       8.9   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       9     0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n       9.5   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0   0   0\n          Reference\nPrediction 5.8 5.9 6 6.1 6.2 6.3 6.4 6.5 6.6 6.7 6.8 6.9 7 7.1 7.2 7.3 7.4 7.5\n       1.7   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       2.3   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       2.4   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       2.5   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       2.7   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       2.8   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       2.9   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.1   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.2   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.3   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.4   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.5   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.6   1   0 0   0   0   0   0   2   0   0   2   0 1   1   1   1   1   1\n       3.7   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.8   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       3.9   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4     0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4.1   0   0 0   0   0   0   0   1   2   0   0   0 1   0   1   0   0   0\n       4.2   0   0 0   0   0   0   0   0   0   0   0   1 0   0   0   0   0   0\n       4.3   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4.4   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4.5   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4.6   0   0 0   1   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4.7   0   0 0   1   1   0   0   0   0   0   0   0 2   0   0   0   0   0\n       4.8   0   0 0   1   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       4.9   0   0 0   0   1   0   0   0   0   0   0   0 0   0   0   0   0   0\n       5     3   0 3   1   1   3   0   0   1   1   0   0 0   0   1   0   0   0\n       5.1   0   0 0   1   0   1   0   0   1   0   0   0 0   0   0   0   1   1\n       5.2   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   1\n       5.3   1   0 0   0   0   1   0   0   0   0   0   0 0   0   0   0   0   0\n       5.4   1   3 1   2   4   2   2   2   5   0   0   0 0   1   0   1   0   0\n       5.5   0   1 0   1   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       5.6   0   0 0   0   0   0   0   2   0   0   1   0 1   0   0   0   1   0\n       5.7   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       5.8   0   0 1   0   4   2   1   0   0   0   1   1 0   0   1   0   2   1\n       5.9   0   0 0   0   0   1   1   1   0   1   1   1 0   0   0   0   0   0\n       6     2   0 0   0   0   1   1   1   0   0   4   1 0   1   0   1   0   0\n       6.1   0   0 0   0   1   3   1   1   4   2   3   2 4   1   0   2   0   0\n       6.2   0   2 4   9   7   4   5   1   7   5   2   1 1   6   1   3   2   2\n       6.3   2   3 1   1   3   2   5   4   3   3   2   3 1   2   3   0   2   0\n       6.4   0   2 3   2   8   4   5   2   5   3   2   2 2   6   4   0   2   0\n       6.5   0   0 1   7   7   4   3   3   2   4   3   2 2   5   2   1   3   0\n       6.6   1   2 1   2   4   0   1   0   0   1   0   2 3   6   0   1   0   1\n       6.7   0   1 0   0   2   0   1   2   6   3   1   2 1   0   3   2   1   1\n       6.8   2   1 0   1   3   0   2   1   2   3   2   0 2   1   0   2   2   1\n       6.9   0   0 0   0   0   0   0   0   1   1   0   0 1   1   1   1   0   0\n       7     1   0 0   0   1   0   1   1   0   1   0   2 0   1   1   1   1   1\n       7.1   0   0 1   1   1   0   0   4   0   0   2   1 0   1   1   2   1   2\n       7.2   1   1 0   1   1   0   3   3   1   1   0   2 1   0   1   1   3   1\n       7.3   0   0 1   0   1   2   0   0   0   2   1   1 1   3   1   1   0   1\n       7.4   0   0 0   0   0   0   0   0   0   0   0   0 0   1   2   0   1   0\n       7.5   0   0 0   1   0   0   0   1   0   1   1   0 0   0   0   1   1   3\n       7.6   0   1 0   1   1   0   0   0   1   0   1   0 0   0   1   0   1   1\n       7.7   0   0 0   0   0   1   0   1   0   0   1   1 0   1   1   2   0   0\n       7.8   0   0 0   0   0   1   0   0   1   0   0   2 0   1   1   0   1   0\n       7.9   1   0 0   0   0   0   0   2   0   0   0   0 2   0   0   0   0   0\n       8     0   0 0   0   0   0   0   1   1   0   0   1 0   1   0   0   1   0\n       8.1   0   0 0   0   0   0   1   1   0   0   1   0 2   1   0   0   0   0\n       8.2   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       8.3   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       8.4   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       8.5   0   0 1   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       8.6   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       8.7   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       8.8   0   0 0   0   0   0   0   0   0   0   0   0 0   1   0   0   0   0\n       8.9   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       9     0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n       9.5   0   0 0   0   0   0   0   0   0   0   0   0 0   0   0   0   0   0\n          Reference\nPrediction 7.6 7.7 7.8 7.9 8 8.1 8.2 8.3 8.4 8.5 8.6 8.7 8.8 8.9 9 9.5\n       1.7   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       2.3   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       2.4   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       2.5   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       2.7   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       2.8   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       2.9   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.1   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.2   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.3   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.4   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.5   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.6   0   1   0   1 1   0   0   0   0   0   0   0   0   0 0   0\n       3.7   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.8   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       3.9   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4     0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.1   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.2   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.3   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.4   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.5   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.6   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.7   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.8   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       4.9   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5     0   0   0   1 0   1   0   0   0   0   0   0   0   0 0   0\n       5.1   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.2   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.3   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.4   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.5   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.6   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.7   0   1   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       5.8   0   0   1   1 0   0   0   0   0   0   0   0   0   0 0   0\n       5.9   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       6     1   0   0   1 0   0   0   0   0   0   0   0   0   0 0   0\n       6.1   0   0   0   0 0   0   0   1   0   0   0   0   0   0 0   0\n       6.2   2   1   2   0 1   0   0   0   0   0   0   0   0   0 0   0\n       6.3   0   0   0   0 0   0   1   0   0   0   0   0   0   0 0   0\n       6.4   1   2   1   0 0   2   0   0   0   0   0   0   0   0 0   0\n       6.5   0   1   1   2 1   0   0   0   0   0   0   0   0   0 0   0\n       6.6   0   1   0   0 0   0   1   0   0   0   0   0   0   0 0   0\n       6.7   0   0   1   0 0   1   0   0   0   0   0   0   0   0 0   0\n       6.8   1   0   1   0 0   0   0   0   0   0   0   0   0   0 0   0\n       6.9   1   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       7     0   0   1   0 2   0   0   0   0   0   0   0   0   0 0   0\n       7.1   0   3   0   0 1   1   0   0   0   0   0   0   0   0 0   0\n       7.2   1   1   0   1 0   0   0   0   0   0   1   0   0   0 0   0\n       7.3   2   0   2   0 1   0   1   1   0   0   0   0   0   0 0   0\n       7.4   2   0   1   1 1   0   0   0   1   0   0   0   0   0 0   0\n       7.5   1   0   0   1 0   1   0   0   0   0   0   0   0   0 0   0\n       7.6   0   1   2   1 1   1   1   0   0   0   0   0   0   0 0   0\n       7.7   1   1   0   0 1   3   0   0   0   0   0   0   0   0 0   0\n       7.8   1   0   1   0 1   0   0   0   0   0   0   0   0   0 0   0\n       7.9   0   0   1   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8     0   0   2   0 1   0   0   0   0   0   0   0   0   0 0   0\n       8.1   0   0   1   2 1   0   0   1   1   0   0   0   0   0 0   0\n       8.2   0   0   1   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8.3   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8.4   0   0   0   0 0   0   0   0   0   1   0   0   0   0 0   0\n       8.5   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8.6   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8.7   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8.8   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       8.9   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       9     0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n       9.5   0   0   0   0 0   0   0   0   0   0   0   0   0   0 0   0\n\nOverall Statistics\n                                          \n               Accuracy : 0.0469          \n                 95% CI : (0.0329, 0.0646)\n    No Information Rate : 0.0684          \n    P-Value [Acc &gt; NIR] : 0.994           \n                                          \n                  Kappa : 0.0106          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: 1.7 Class: 2.3 Class: 2.4 Class: 2.5 Class: 2.7\nSensitivity                  NA         NA    0.00000    0.00000    0.00000\nSpecificity                   1          1    1.00000    1.00000    1.00000\nPos Pred Value               NA         NA        NaN        NaN        NaN\nNeg Pred Value               NA         NA    0.99866    0.99866    0.99866\nPrevalence                    0          0    0.00134    0.00134    0.00134\nDetection Rate                0          0    0.00000    0.00000    0.00000\nDetection Prevalence          0          0    0.00000    0.00000    0.00000\nBalanced Accuracy            NA         NA    0.50000    0.50000    0.50000\n                     Class: 2.8 Class: 2.9 Class: 3.1 Class: 3.2 Class: 3.3\nSensitivity             0.00000         NA         NA    0.00000         NA\nSpecificity             1.00000          1          1    1.00000          1\nPos Pred Value              NaN         NA         NA        NaN         NA\nNeg Pred Value          0.99866         NA         NA    0.99866         NA\nPrevalence              0.00134          0          0    0.00134          0\nDetection Rate          0.00000          0          0    0.00000          0\nDetection Prevalence    0.00000          0          0    0.00000          0\nBalanced Accuracy       0.50000         NA         NA    0.50000         NA\n                     Class: 3.4 Class: 3.5 Class: 3.6 Class: 3.7 Class: 3.8\nSensitivity             0.00000   0.000000    0.00000         NA         NA\nSpecificity             1.00000   1.000000    0.96913          1          1\nPos Pred Value              NaN        NaN    0.00000         NA         NA\nNeg Pred Value          0.99866   0.997319    0.99862         NA         NA\nPrevalence              0.00134   0.002681    0.00134          0          0\nDetection Rate          0.00000   0.000000    0.00000          0          0\nDetection Prevalence    0.00000   0.000000    0.03083          0          0\nBalanced Accuracy       0.50000   0.500000    0.48456         NA         NA\n                     Class: 3.9 Class: 4 Class: 4.1 Class: 4.2 Class: 4.3\nSensitivity                  NA 0.000000   0.000000    0.00000   0.000000\nSpecificity                   1 1.000000   0.993271    0.99866   1.000000\nPos Pred Value               NA      NaN   0.000000    0.00000        NaN\nNeg Pred Value               NA 0.997319   0.995951    0.99866   0.993298\nPrevalence                    0 0.002681   0.004021    0.00134   0.006702\nDetection Rate                0 0.000000   0.000000    0.00000   0.000000\nDetection Prevalence          0 0.000000   0.006702    0.00134   0.000000\nBalanced Accuracy            NA 0.500000   0.496635    0.49933   0.500000\n                     Class: 4.4 Class: 4.5 Class: 4.6 Class: 4.7 Class: 4.8\nSensitivity            0.000000   0.000000   0.000000   0.000000    0.00000\nSpecificity            1.000000   1.000000   0.994616   0.991914    0.99866\nPos Pred Value              NaN        NaN   0.000000   0.000000    0.00000\nNeg Pred Value         0.995979   0.995979   0.995957   0.994595    0.99866\nPrevalence             0.004021   0.004021   0.004021   0.005362    0.00134\nDetection Rate         0.000000   0.000000   0.000000   0.000000    0.00000\nDetection Prevalence   0.000000   0.000000   0.005362   0.008043    0.00134\nBalanced Accuracy      0.500000   0.500000   0.497308   0.495957    0.49933\n                     Class: 4.9 Class: 5 Class: 5.1 Class: 5.2 Class: 5.3\nSensitivity            0.000000 0.153846   0.000000   0.000000   0.000000\nSpecificity            0.998656 0.964529   0.993234   0.993225   0.995918\nPos Pred Value         0.000000 0.071429   0.000000   0.000000   0.000000\nNeg Pred Value         0.997315 0.984680   0.990553   0.989204   0.985195\nPrevalence             0.002681 0.017426   0.009383   0.010724   0.014745\nDetection Rate         0.000000 0.002681   0.000000   0.000000   0.000000\nDetection Prevalence   0.001340 0.037534   0.006702   0.006702   0.004021\nBalanced Accuracy      0.499328 0.559188   0.496617   0.496612   0.497959\n                     Class: 5.4 Class: 5.5 Class: 5.6 Class: 5.7 Class: 5.8\nSensitivity            0.142857   0.000000    0.00000   0.000000    0.00000\nSpecificity            0.955345   0.997294    0.98638   0.994521    0.97671\nPos Pred Value         0.029412   0.000000    0.00000   0.000000    0.00000\nNeg Pred Value         0.991573   0.990591    0.98370   0.978437    0.97805\nPrevalence             0.009383   0.009383    0.01609   0.021448    0.02145\nDetection Rate         0.001340   0.000000    0.00000   0.000000    0.00000\nDetection Prevalence   0.045576   0.002681    0.01340   0.005362    0.02279\nBalanced Accuracy      0.549101   0.498647    0.49319   0.497260    0.48836\n                     Class: 5.9 Class: 6 Class: 6.1 Class: 6.2 Class: 6.3\nSensitivity            0.000000  0.00000    0.00000   0.137255   0.062500\nSpecificity            0.991770  0.97665    0.96208   0.899281   0.925770\nPos Pred Value         0.000000  0.00000    0.00000   0.090909   0.036364\nNeg Pred Value         0.977027  0.97531    0.95271   0.934230   0.956585\nPrevalence             0.022788  0.02413    0.04558   0.068365   0.042895\nDetection Rate         0.000000  0.00000    0.00000   0.009383   0.002681\nDetection Prevalence   0.008043  0.02279    0.03619   0.103217   0.073727\nBalanced Accuracy      0.495885  0.48832    0.48104   0.518268   0.494135\n                     Class: 6.4 Class: 6.5 Class: 6.6 Class: 6.7 Class: 6.8\nSensitivity            0.151515   0.081081    0.00000   0.093750   0.064516\nSpecificity            0.894811   0.923836    0.95733   0.960784   0.963636\nPos Pred Value         0.062500   0.052632    0.00000   0.096774   0.071429\nNeg Pred Value         0.957958   0.950653    0.93994   0.959441   0.959610\nPrevalence             0.044236   0.049598    0.05764   0.042895   0.041555\nDetection Rate         0.006702   0.004021    0.00000   0.004021   0.002681\nDetection Prevalence   0.107239   0.076408    0.04021   0.041555   0.037534\nBalanced Accuracy      0.523163   0.502459    0.47866   0.527267   0.514076\n                     Class: 6.9 Class: 7 Class: 7.1 Class: 7.2 Class: 7.3\nSensitivity            0.000000  0.00000    0.02381    0.03704    0.04348\nSpecificity            0.990251  0.97911    0.97017    0.96384    0.96957\nPos Pred Value         0.000000  0.00000    0.04545    0.03704    0.04348\nNeg Pred Value         0.962111  0.96170    0.94337    0.96384    0.96957\nPrevalence             0.037534  0.03753    0.05630    0.03619    0.03083\nDetection Rate         0.000000  0.00000    0.00134    0.00134    0.00134\nDetection Prevalence   0.009383  0.02011    0.02949    0.03619    0.03083\nBalanced Accuracy      0.495125  0.48955    0.49699    0.50044    0.50652\n                     Class: 7.4 Class: 7.5 Class: 7.6 Class: 7.7 Class: 7.8\nSensitivity             0.03704   0.166667    0.00000    0.07692    0.05263\nSpecificity             0.98609   0.986264    0.97951    0.98226    0.98624\nPos Pred Value          0.09091   0.230769    0.00000    0.07143    0.09091\nNeg Pred Value          0.96463   0.979536    0.98085    0.98361    0.97551\nPrevalence              0.03619   0.024129    0.01877    0.01743    0.02547\nDetection Rate          0.00134   0.004021    0.00000    0.00134    0.00134\nDetection Prevalence    0.01475   0.017426    0.02011    0.01877    0.01475\nBalanced Accuracy       0.51156   0.576465    0.48975    0.52959    0.51944\n                     Class: 7.9 Class: 8 Class: 8.1 Class: 8.2 Class: 8.3\nSensitivity            0.000000  0.07692    0.00000   0.000000   0.000000\nSpecificity            0.990463  0.99045    0.98370   0.998652   1.000000\nPos Pred Value         0.000000  0.12500    0.00000   0.000000        NaN\nNeg Pred Value         0.983762  0.98374    0.98638   0.994631   0.995979\nPrevalence             0.016086  0.01743    0.01340   0.005362   0.004021\nDetection Rate         0.000000  0.00134    0.00000   0.000000   0.000000\nDetection Prevalence   0.009383  0.01072    0.01609   0.001340   0.000000\nBalanced Accuracy      0.495232  0.53369    0.49185   0.499326   0.500000\n                     Class: 8.4 Class: 8.5 Class: 8.6 Class: 8.7 Class: 8.8\nSensitivity            0.000000    0.00000    0.00000         NA         NA\nSpecificity            0.998656    0.99866    1.00000          1    0.99866\nPos Pred Value         0.000000    0.00000        NaN         NA         NA\nNeg Pred Value         0.997315    0.99866    0.99866         NA         NA\nPrevalence             0.002681    0.00134    0.00134          0    0.00000\nDetection Rate         0.000000    0.00000    0.00000          0    0.00000\nDetection Prevalence   0.001340    0.00134    0.00000          0    0.00134\nBalanced Accuracy      0.499328    0.49933    0.50000         NA         NA\n                     Class: 8.9 Class: 9 Class: 9.5\nSensitivity                  NA       NA         NA\nSpecificity                   1        1          1\nPos Pred Value               NA       NA         NA\nNeg Pred Value               NA       NA         NA\nPrevalence                    0        0          0\nDetection Rate                0        0          0\nDetection Prevalence          0        0          0\nBalanced Accuracy            NA       NA         NA\n\n\n\naccuracy &lt;- cm$overall['Accuracy']\nprecision &lt;- cm$byClass['Precision']\nrecall &lt;- cm$byClass['Recall']\nF1 &lt;- 2 * (precision * recall) / (precision + recall)\naccuracy\n\n  Accuracy \n0.04691689 \n\nprecision\n\n[1] NA\n\nrecall\n\n[1] NA\n\nF1\n\n[1] NA\n\n\n\nlibrary(ggplot2)\nlibrary(reshape2) \n\nnumeric_data &lt;- filter_data[, sapply(filter_data, is.numeric)]\ncor_matrix &lt;- cor(numeric_data, use = \"complete.obs\")\n\nmelted_cormat &lt;- melt(cor_matrix)\nggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + \n  geom_tile() +\n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", \n                       midpoint=0, limit=c(-1,1), space=\"Lab\", \n                       name=\"Correlation\") +\n  theme_minimal() + \n  labs(title = \"Correlation Heatmap\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                    size = 10, hjust = 1))\n\n\n\n\n\nggplot(filter_data, aes(x=Runtime, y=Rating)) +\n  geom_point(alpha=0.7) +\n  labs(title=\"Scatter plot of Runtime vs. Rating\", x=\"Runtime\", y=\"Rating\")\n\n\n\n\n\nfilter_data$Year &lt;- (filter_data$Year - min(filter_data$Year)) / (max(filter_data$Year) - min(filter_data$Year))\n\nggplot(filter_data, aes(x=Year, y=Rating)) +\n  geom_point(alpha=0.7) +\n  labs(title=\"Scatter plot of Year vs. Rating\", x=\"Year\", y=\"Rating\")\n\n\n\n\n\nlibrary(ggplot2)\n\ncomparison &lt;- as.data.frame(cbind(Actual=test_data$Rating, Predicted=predictions))\nggplot(comparison, aes(x=Actual, y=Predicted)) + geom_point(aes(color=Actual)) + \n  ggtitle('Actual vs Predicted') + theme_minimal()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nDiscuss the concepts of overfitting and under-fitting and whether your model is doing it.\nI’ve added as many relevant features as I can, and the model has close to 4,000 records, but the accuracy is very low, suggesting that the correlation between these features is very low. I will try to find more relevant datasets to repeat this question in subsequent assignments.\nDiscuss the model’s performance in terms of accuracy and other relevant metrics.\nThe accuracy is too low causing several other metrics of my model to show NA, I will try to find a more compliant dataset in subsequent assignments\nConclusion\nBy analyzing the movie data using a plain Bayesian model, we obtained some interesting findings. First, the accuracy of the model may not be as high as we expect, suggesting that our data features may not fully capture all the important information that determines movie ratings. In addition, the heatmap of the confusion matrix shows that the model predicts certain ratings more accurately than others. The distribution plots of the features further reveal possible trends between specific ratings and certain features, which provides insight into the relationship between movie ratings and these features. While the performance of the model could be improved, this analysis provides us with a benchmark that could be considered in the future to further optimize model performance using more complex models or more features. Overall, this study provides an initial understanding of the relationship between movie ratings and their features and provides a solid foundation for future in-depth research."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "This is my data link:\nhttps://github.com/anly501/dsan-5000-project-962570185/tree/main/data"
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Get data\nimport pandas as pd\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\nfile_name = '../../../data/others/IMDB_Top_250_Movies_ver2.csv'\ndf=pd.read_csv(file_name)  \ndf = df.drop(['Unnamed: 0'], axis=1)\nprint(df.shape)\n\n# genre\nmlb = MultiLabelBinarizer()\ndf_genres = mlb.fit_transform(df['genre'])\ndf_genres = pd.DataFrame(df_genres, columns=mlb.classes_)\ndf = df.drop('genre', axis=1).join(df_genres)\n\n#casts\ndf['casts'] = df['casts'].apply(lambda x: x.split(','))\ndf_casts = mlb.fit_transform(df['casts'])\ndf_casts = pd.DataFrame(df_casts, columns=mlb.classes_)\ndf = df.drop('casts', axis=1).join(df_casts)\n\n# directors\nencoder = OneHotEncoder(sparse=False)\nct = ColumnTransformer([(\"directors\", encoder, ['directors'])], remainder='passthrough')\ndf_encoded = ct.fit_transform(df)\ndf_encoded = pd.DataFrame(df_encoded, columns=ct.get_feature_names_out())\ndf = df.drop('directors', axis=1)\ndf = pd.concat([df, df_encoded], axis=1)\n\n\nprint(df.head())\n\nX = np.array(df)\n\n(250, 5)\n   year  rating  ,  -  A  B  C  D  F  H  ...  remainder__Yuriko Ishida  \\\n0  1994     9.3  0  0  0  0  0  1  0  0  ...                       0.0   \n1  1972     9.2  1  0  0  0  1  1  0  0  ...                       0.0   \n2  2008     9.0  1  0  1  0  1  1  0  0  ...                       0.0   \n3  1974     9.0  1  0  0  0  1  1  0  0  ...                       0.0   \n4  1957     9.0  1  0  0  0  1  1  0  0  ...                       0.0   \n\n   remainder__Yuriy Solomin  remainder__Yutaka Sada  remainder__Yves Montand  \\\n0                       0.0                     0.0                      0.0   \n1                       0.0                     0.0                      0.0   \n2                       0.0                     0.0                      0.0   \n3                       0.0                     0.0                      0.0   \n4                       0.0                     0.0                      0.0   \n\n   remainder__Yôji Matsuda  remainder__Yûko Tanaka  remainder__Zain Al Rafeea  \\\n0                      0.0                     0.0                        0.0   \n1                      0.0                     0.0                        0.0   \n2                      0.0                     0.0                        0.0   \n3                      0.0                     0.0                        0.0   \n4                      0.0                     0.0                        0.0   \n\n   remainder__Zazie Beetz  remainder__Zendaya  remainder__Çetin Tekindor  \n0                     0.0                 0.0                        0.0  \n1                     0.0                 0.0                        0.0  \n2                     0.0                 0.0                        0.0  \n3                     0.0                 0.0                        0.0  \n4                     0.0                 0.0                        0.0  \n\n[5 rows x 1404 columns]\n\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning:\n\n`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "This is my code link:\nhttps://github.com/anly501/dsan-5000-project-962570185"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "This is link to article1:\nhttps://www.sciencedirect.com/science/article/pii/S0957417413006866\nAbstract:This research aims to predict movie box office outcomes using data from microblogs. Unlike prior studies that relied merely on the count of related microblogs, this study delves deeper, utilizing both count-based and content-based features from social media. Count-based features utilize user information to reduce the impact of irrelevant microblogs, while content-based features apply a new box office-oriented semantic classification to enhance relevance. Advanced machine learning models, including SVM and neural networks, are employed. The model’s efficacy is tested using data from Tencent microblog to predict box offices for specific movies in China, demonstrating the predictive power of online social media.\nThis is link to article2:\nhttps://www.tandfonline.com/doi/full/10.1080/13504851.2018.1441499?scroll=top&needAccess=true\nIn this note, we contrast prediction performance of nine econometric and machine learning methods, including a new hybrid method combining model averaging and machine learning, using data from the film industry and social media. The results suggest that machine learning methods have an advantage in addressing short-run noise, whereas traditional econometric methods are better at capturing long-run trend. In addition, once sample heterogeneity is controlled, the new hybrid method tends to strike a right balance in dealing with both noise and trend, leading to superior prediction efficiency.\nThese are my 10 questions:\n\nFeature Importance: Which factors (e.g., trailer views, social media buzz, director/actor popularity) have the most significant impact on the accuracy of box office predictions?\nTime Sensitivity: Which time period leading up to the movie’s release (e.g., the first week, first month, first quarter) provides the most predictive value for box office results?\nModel Selection: Which machine learning or statistical model (e.g., linear regression, decision trees, neural networks) performs best for box office predictions?\nExternal Event Influences: How can external events (e.g., concurrent releases of other major films, significant news events, holidays) that might affect the box office be quantified and incorporated into predictions?\nRegional Variations: Are there significant differences in movie preferences among audiences from different regions (e.g., Asia, North America, Europe)? How should prediction models be adjusted to account for these differences?\nSentiment Analysis of Social Media: How does the sentiment (positive, neutral, negative) of audience discussions on social media platforms influence box office revenues?\nPrediction Timeliness: How does the accuracy of box office predictions change over time? How does the prediction accuracy in the first week after the movie’s release compare to the accuracy before the release?\nBudget vs. Revenue: What is the relationship between a movie’s production budget and its box office revenue? Does a higher budget always guarantee higher box office returns?\nHistorical Data Influence: How do the past box office records of a director, actor, or production company influence the predictions for their new movies?\nInfluence of Specific Genres or Themes: Are certain types of movies (e.g., sci-fi, romance, adventure) easier to predict in terms of box office? Do different genres require different prediction models or features?"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "See my whole eda file in 5000-website/eda folder, I have three files for my three data sets"
  },
  {
    "objectID": "eda/eda_cleaned_data_py1.html",
    "href": "eda/eda_cleaned_data_py1.html",
    "title": "Untitled",
    "section": "",
    "text": "# data &lt;- read.csv(\"../../.././data./01-modified-data./cleaned_data_py1.csv\")\ndata &lt;- read.csv(\"../../../data/01-modified-data/cleaned_data_py1.csv\")\n\n\nhead(data)\n\n       id            title\n1  926393  The Equalizer 3\n2 1008042       Talk to Me\n3  968051       The Nun II\n4 1151534          Nowhere\n5  678512 Sound of Freedom\n6  937249       57 Seconds\n                                                                                                                                                                                                                                      overview\n1 Robert McCall finds himself at home in Southern Italy but he discovers his friends are under the control of local crime bosses. As events turn deadly, McCall knows what he has to do: become his friends' protector by taking on the mafia.\n2                                           When a group of friends discover how to conjure spirits using an embalmed hand, they become hooked on the new thrill, until one of them goes too far and unleashes terrifying supernatural forces.\n3                                                                                              In 1956 France, a priest is violently murdered, and Sister Irene begins to investigate. She once again comes face-to-face with a powerful evil.\n4                A young pregnant woman named Mia escapes from a country at war by hiding in a maritime container aboard a cargo ship. After a violent storm, Mia gives birth to the child while lost at sea, where she must fight to survive.\n5                                                                                      The story of Tim Ballard, a former US government agent, who quits his job in order to devote his life to rescuing children from global sex traffickers.\n6                                                                                 When a tech blogger lands an interview with a tech guru and stops an attack on him, he finds a mysterious ring that takes him back 57 seconds into the past.\n  popularity release_date vote_average vote_count\n1   3197.130   2023-08-30          7.3        671\n2   2629.730   2023-07-26          7.3       1235\n3   2591.014   2023-09-06          7.0        849\n4   2140.286   2023-09-29          7.7        523\n5   1898.501   2023-07-03          8.1        789\n6   1528.116   2023-09-29          5.7         54\n\n\n\nsummary(data)\n\n       id             title             overview           popularity     \n Min.   :     11   Length:1982        Length:1982        Min.   :  35.80  \n 1st Qu.:  21447   Class :character   Class :character   1st Qu.:  41.96  \n Median : 323082   Mode  :character   Mode  :character   Median :  51.96  \n Mean   : 395937                                         Mean   :  82.55  \n 3rd Qu.: 705404                                         3rd Qu.:  72.83  \n Max.   :1183387                                         Max.   :3197.13  \n release_date        vote_average      vote_count     \n Length:1982        Min.   : 0.000   Min.   :    0.0  \n Class :character   1st Qu.: 6.200   1st Qu.:  192.2  \n Mode  :character   Median : 6.800   Median : 2007.5  \n                    Mean   : 6.639   Mean   : 4038.2  \n                    3rd Qu.: 7.400   3rd Qu.: 5843.0  \n                    Max.   :10.000   Max.   :34558.0  \n\n\n\nstr(data)\n\n'data.frame':   1982 obs. of  7 variables:\n $ id          : int  926393 1008042 968051 1151534 678512 937249 980489 575264 565770 893723 ...\n $ title       : chr  \"The Equalizer 3\" \"Talk to Me\" \"The Nun II\" \"Nowhere\" ...\n $ overview    : chr  \"Robert McCall finds himself at home in Southern Italy but he discovers his friends are under the control of loc\"| __truncated__ \"When a group of friends discover how to conjure spirits using an embalmed hand, they become hooked on the new t\"| __truncated__ \"In 1956 France, a priest is violently murdered, and Sister Irene begins to investigate. She once again comes fa\"| __truncated__ \"A young pregnant woman named Mia escapes from a country at war by hiding in a maritime container aboard a cargo\"| __truncated__ ...\n $ popularity  : num  3197 2630 2591 2140 1899 ...\n $ release_date: chr  \"2023-08-30\" \"2023-07-26\" \"2023-09-06\" \"2023-09-29\" ...\n $ vote_average: num  7.3 7.3 7 7.7 8.1 5.7 8.1 7.8 7.1 7.7 ...\n $ vote_count  : int  671 1235 849 523 789 54 965 1351 1234 34 ...\n\n\n\ncolSums(is.na(data))\n\n          id        title     overview   popularity release_date vote_average \n           0            0            0            0            0            0 \n  vote_count \n           0 \n\n\n\ncor(data[, sapply(data, is.numeric)])\n\n                     id  popularity vote_average  vote_count\nid            1.0000000  0.18598981  -0.15729903 -0.46799809\npopularity    0.1859898  1.00000000   0.06951356 -0.04481013\nvote_average -0.1572990  0.06951356   1.00000000  0.34562377\nvote_count   -0.4679981 -0.04481013   0.34562377  1.00000000\n\n\n\nmeans &lt;- sapply(data[, sapply(data, is.numeric)], mean, na.rm=TRUE)\n\nmedians &lt;- sapply(data[, sapply(data, is.numeric)], median, na.rm=TRUE)\n\nstd_devs &lt;- sapply(data[, sapply(data, is.numeric)], sd, na.rm=TRUE)\n\nvariances &lt;- sapply(data[, sapply(data, is.numeric)], var, na.rm=TRUE)\n\nprint(list(Means = means, Medians = medians, Standard_Deviations = std_devs, Variances = variances))\n\n$Means\n          id   popularity vote_average   vote_count \n3.959370e+05 8.254565e+01 6.638698e+00 4.038202e+03 \n\n$Medians\n          id   popularity vote_average   vote_count \n  323082.000       51.957        6.800     2007.500 \n\n$Standard_Deviations\n          id   popularity vote_average   vote_count \n3.760296e+05 1.599089e+02 1.314091e+00 5.217476e+03 \n\n$Variances\n          id   popularity vote_average   vote_count \n1.413983e+11 2.557085e+04 1.726836e+00 2.722205e+07 \n\n\n\ncategorical_vars &lt;- data[, sapply(data, is.factor)]\n\nfreq_distributions &lt;- lapply(categorical_vars, function(x) {\n  tbl &lt;- table(x)\n  prop &lt;- prop.table(tbl)\n  data.frame(Category = names(tbl), Frequency = as.numeric(tbl), Proportion = prop)\n})\n\nprint(freq_distributions)\n\nnamed list()\n\n\n\nlibrary(ggplot2)\n\nlapply(names(categorical_vars), function(var_name) {\n  ggplot(data, aes_string(x = var_name)) + \n    geom_bar() +\n    labs(title = paste(\"Bar chart of\", var_name), y = \"Frequency\") +\n    theme_minimal()\n})\n\nlist()\n\n\n\n# Histogram for popularity\nggplot(data, aes(x=popularity)) + \n  geom_histogram(binwidth=10, fill=\"blue\", color=\"black\", alpha=0.7) + \n  labs(title=\"Histogram of Popularity\", x=\"Value\", y=\"Frequency\")\n\n\n\n# Histogram for vote_average\nggplot(data, aes(x=vote_average)) + \n  geom_histogram(binwidth=0.5, fill=\"blue\", color=\"black\", alpha=0.7) + \n  labs(title=\"Histogram of Vote Average\", x=\"Value\", y=\"Frequency\")\n\n\n\n# Histogram for vote_count\nggplot(data, aes(x=vote_count)) + \n  geom_histogram(binwidth=50, fill=\"blue\", color=\"black\", alpha=0.7) + \n  labs(title=\"Histogram of Vote Count\", x=\"Value\", y=\"Frequency\")\n\n\n\n\n\n# Boxplot for popularity\nggplot(data, aes(y=popularity)) + \n  geom_boxplot(fill=\"blue\", color=\"black\", alpha=0.7) + \n  labs(title=\"Boxplot of Popularity\", y=\"Value\")\n\n\n\n# Boxplot for vote_average\nggplot(data, aes(y=vote_average)) + \n  geom_boxplot(fill=\"blue\", color=\"black\", alpha=0.7) + \n  labs(title=\"Boxplot of Vote Average\", y=\"Value\")\n\n\n\n# Boxplot for vote_count\nggplot(data, aes(y=vote_count)) + \n  geom_boxplot(fill=\"blue\", color=\"black\", alpha=0.7) + \n  labs(title=\"Boxplot of Vote Count\", y=\"Value\")\n\n\n\n\n\n# Scatter plot of popularity vs vote_average\nggplot(data, aes(x=popularity, y=vote_average)) + \n  geom_point(alpha=0.7) + \n  labs(title=\"Scatter plot of Popularity vs Vote Average\", x=\"Popularity\", y=\"Vote Average\")\n\n\n\n# Scatter plot of popularity vs vote_count\nggplot(data, aes(x=popularity, y=vote_count)) + \n  geom_point(alpha=0.7) + \n  labs(title=\"Scatter plot of Popularity vs Vote Count\", x=\"Popularity\", y=\"Vote Count\")\n\n\n\n# Scatter plot of vote_average vs vote_count\nggplot(data, aes(x=vote_average, y=vote_count)) + \n  geom_point(alpha=0.7) + \n  labs(title=\"Scatter plot of Vote Average vs Vote Count\", x=\"Vote Average\", y=\"Vote Count\")\n\n\n\n\n\ncor_matrix &lt;- cor(data[,c(\"popularity\", \"vote_average\", \"vote_count\")], use=\"complete.obs\")\nlibrary(reshape2)\ncor_melted &lt;- melt(cor_matrix)\n\nggplot(cor_melted, aes(x=Var1, y=Var2)) + \n  geom_tile(aes(fill=value), color=\"white\") + \n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", midpoint=0, limit=c(-1,1)) + \n  geom_text(aes(label=round(value, 2)), vjust=1) + \n  labs(title=\"Heatmap of correlation matrix\") + \n  theme_minimal() + \n  theme(axis.text.x=element_text(angle=45, vjust=1, hjust=1))\n\n\n\n\n\ncor_matrix &lt;- cor(data[,c(\"popularity\", \"vote_average\", \"vote_count\")], use=\"complete.obs\")\nprint(cor_matrix)\n\n              popularity vote_average  vote_count\npopularity    1.00000000   0.06951356 -0.04481013\nvote_average  0.06951356   1.00000000  0.34562377\nvote_count   -0.04481013   0.34562377  1.00000000\n\n\nHypothesis Generation.\n1. Movie popularity is positively related to the number of votes cast: It is hypothesized that popular movies may receive more votes because more viewers may have seen and rated them.\n2. Average vote score is positively related to the number of votes: High quality movies may attract more viewers and thus get more votes. However, it is also possible that even if a movie receives many votes, its average score is not necessarily high.\n3. Recently released movies may have higher popularity: Recently released movies may have high popularity because they are likely to be hot topics of interest to the audience.\n4. The length of the synopsis is not significantly related to popularity: The synopsis of a movie may not have a direct effect on the popularity of the movie, however, if the synopsis is able to attract the audience, then it may have an indirect effect on the popularity of the movie.\nResearch Question.\n1. which genre of movie is the most popular? By looking at other variables in the dataset, such as movie genre or director, we can further explore which genre of movie is most popular with audiences.\n2. does popularity change over time? Are there certain years or seasons where movies released during those periods are more popular?\n3. what are the common characteristics of highly rated movies? Is it the director, the actors, the genre or other factors?\n4. is the length of a movie’s synopsis related to its popularity? Do longer synopses attract more viewers than shorter ones?\n\ndata$rating_group &lt;- cut(data$vote_average, breaks = c(0, 5, 7, 10), labels = c(\"Low\", \"Medium\", \"High\"))\n\n\ndata$release_year &lt;- as.numeric(format(as.Date(data$release_date, format=\"%Y-%m-%d\"), \"%Y\"))\ndata$release_month &lt;- as.numeric(format(as.Date(data$release_date, format=\"%Y-%m-%d\"), \"%m\"))\n\n\ndata$popularity_group &lt;- cut(data$popularity, breaks = quantile(data$popularity, c(0, .33, .66, 1)), labels = c(\"Low\", \"Medium\", \"High\"))\n\n\ndata$vote_count_group &lt;- cut(data$vote_count, breaks = quantile(data$vote_count, c(0, .33, .66, 1)), labels = c(\"Low\", \"Medium\", \"High\"))\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata %&gt;% \n  group_by(rating_group) %&gt;% \n  summarise(\n    avg_popularity = mean(popularity, na.rm = TRUE),\n    median_vote_count = median(vote_count, na.rm = TRUE),\n    count = n()\n  )\n\n# A tibble: 4 × 4\n  rating_group avg_popularity median_vote_count count\n  &lt;fct&gt;                 &lt;dbl&gt;             &lt;dbl&gt; &lt;int&gt;\n1 Low                    58.0               12     94\n2 Medium                 73.5             1700   1085\n3 High                   99.2             4188.   766\n4 &lt;NA&gt;                   65.8                0     37\n\n\n\nlibrary(ggplot2)\n\nggplot(data, aes(x = rating_group, y = popularity)) +\n  geom_boxplot() +\n  ggtitle(\"Popularity by Rating Group\")\n\n\n\n\n\nlibrary(stats)\nresult &lt;- aov(popularity ~ rating_group, data = data)\nsummary(result)\n\n               Df   Sum Sq Mean Sq F value   Pr(&gt;F)    \nrating_group    2   359574  179787    6.95 0.000983 ***\nResiduals    1942 50237711   25869                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n37 observations deleted due to missingness\n\n\n\ntable(data$rating_group, data$popularity_group)\n\n        \n         Low Medium High\n  Low     38     35   21\n  Medium 393    367  324\n  High   206    245  315\n\n\n\nggplot(data, aes(x = release_year, y = popularity, color = rating_group)) +\n  geom_line() +\n  ggtitle(\"Popularity Trend by Rating Group over Years\")\n\n\n\n\n\nQ1 &lt;- quantile(data$popularity, 0.25)\nQ3 &lt;- quantile(data$popularity, 0.75)\nIQR &lt;- Q3 - Q1\n\nlower_bound &lt;- Q1 - 1.5 * IQR\nupper_bound &lt;- Q3 + 1.5 * IQR\n\noutliers_popularity &lt;- data$popularity[data$popularity &lt; lower_bound | data$popularity &gt; upper_bound]\noutliers_popularity\n\n  [1] 3197.130 2629.730 2591.014 2140.286 1898.501 1528.116 1488.885 1345.436\n  [9] 1249.748 1188.579 1156.859 1047.253  928.222  920.871  863.039  744.608\n [17]  703.403  667.115  661.903  643.031  621.191  619.533  548.105  542.885\n [25]  486.116  460.358  453.163  438.540  422.105  411.553  402.118  393.774\n [33]  391.079  384.244  382.382  376.836  361.781  359.522  346.609  340.517\n [41]  327.911  318.477  312.994  309.374  300.290  289.525  284.073  282.099\n [49]  277.854  277.360  275.620  271.402  267.984  266.320  255.733  252.839\n [57]  251.363  250.624  248.197  243.964  241.807  241.795  240.890  240.619\n [65]  239.292  235.687  231.485  224.197  223.576  220.415  220.289  219.728\n [73]  216.617  214.690  214.462  211.206  210.893  209.999  209.174  206.842\n [81]  205.109  200.720  200.269  200.131  199.809  199.228  196.169  195.851\n [89]  193.709  193.662  193.398  193.281  192.722  192.675  190.778  190.574\n [97]  187.248  187.028  186.310  184.606  184.290  184.286  183.826  182.224\n[105]  180.598  179.884  176.557  175.960  175.593  174.494  173.339  173.098\n[113]  172.975  172.255  171.929  170.308  169.877  169.209  168.921  167.259\n[121]  166.065  165.855  164.639  163.855  163.107  162.270  161.424  161.321\n[129]  160.349  158.141  157.605  155.794  155.560  155.545  154.586  153.940\n[137]  152.617  152.357  150.789  148.791  148.687  148.640  148.265  148.019\n[145]  147.828  147.604  147.295  146.598  146.006  145.759  145.149  143.969\n[153]  143.590  142.375  141.989  141.534  140.492  137.783  137.258  137.092\n[161]  136.370  134.928  134.928  133.878  133.624  133.447  133.077  133.058\n[169]  132.928  132.279  130.910  130.664  130.388  130.328  129.343  129.087\n[177]  128.800  128.724  127.945  127.516  127.210  126.936  126.795  126.787\n[185]  126.684  126.329  125.688  125.190  125.067  124.839  124.086  123.709\n[193]  123.622  123.601  123.571  123.421  123.077  122.617  121.777  121.172\n[201]  120.699  120.614  120.233  120.127  119.955\n\n\n\nQ1_vote_avg &lt;- quantile(data$vote_average, 0.25)\nQ3_vote_avg &lt;- quantile(data$vote_average, 0.75)\nIQR_vote_avg &lt;- Q3_vote_avg - Q1_vote_avg\n\nlower_bound_vote_avg &lt;- Q1_vote_avg - 1.5 * IQR_vote_avg\nupper_bound_vote_avg &lt;- Q3_vote_avg + 1.5 * IQR_vote_avg\n\noutliers_vote_avg &lt;- data$vote_average[data$vote_average &lt; lower_bound_vote_avg | data$vote_average &gt; upper_bound_vote_avg]\noutliers_vote_avg\n\n [1]  0.0  0.0  0.0  0.0  3.9  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  9.8\n[16]  3.5  0.0  0.0  0.0  4.3  4.3  4.2  4.0  2.0  4.2  3.7  2.0 10.0  3.6  0.0\n[31]  4.3  0.0  0.0  0.0  3.9  0.0  9.5  4.0  4.3  0.0  4.1  0.0  3.0  4.2  9.5\n[46]  0.0  4.0  0.0  4.2  4.2 10.0  0.0  0.0  3.5  3.0  0.0  4.0  1.0  3.8  2.0\n[61]  0.0  0.0  0.0  1.0  0.0  0.0  0.0  4.3  0.0  1.0  0.0  0.0  0.0  0.0  4.0\n\n\n\nQ1_vote_count &lt;- quantile(data$vote_count, 0.25)\nQ3_vote_count &lt;- quantile(data$vote_count, 0.75)\nIQR_vote_count &lt;- Q3_vote_count - Q1_vote_count\n\nlower_bound_vote_count &lt;- Q1_vote_count - 1.5 * IQR_vote_count\nupper_bound_vote_count &lt;- Q3_vote_count + 1.5 * IQR_vote_count\n\noutliers_vote_count &lt;- data$vote_count[data$vote_count &lt; lower_bound_vote_count | data$vote_count &gt; upper_bound_vote_count]\noutliers_vote_count\n\n  [1] 17805 27762 18338 25432 15262 19218 20081 20432 32644 18166 19326 17818\n [13] 17221 18739 29200 17030 15804 15927 23901 18220 18898 28925 15548 19267\n [25] 22277 17409 15764 19563 19503 24708 34558 17196 29855 23370 23682 23472\n [37] 30690 18100 19781 21568 14749 24909 14894 14881 19193 14515 18051 20529\n [49] 14755 15842 20384 17102 14708 17682 20960 22390 21784 14946 15999 16478\n [61] 20312 16345 23880 16375 14455 16123 14432 21091 17482 25961 16363 17383\n [73] 17004 14335 25471 18980 20051 20943 27300 15985 14637 21373 20790 21080\n [85] 14745 18139 16249 14684 19447 18381 14450 16617 19441 18548 16381 17145\n [97] 18756 14655 22363 17954 18489 17913 15687 14376 14682 24713 16593 14329\n[109] 19603 14418 16202 20124 16877 14480 16108 15200 14750 15486 14731 16211\n[121] 14615 20296 26681 15653\n\n\nAnswer for 8\n1. Overview of methods.\nIn performing Exploratory Data Analysis (EDA), we use a range of techniques and methods to gain insight into the data set. Initial steps include data understanding, descriptive statistics, data visualization, correlation analysis, hypothesis generation, data grouping and segmentation, and outlier detection.\n2. Key Findings.\nDescriptive Statistics: We began by analyzing the core features of the dataset with descriptive statistics. This includes statistical information such as mean, median, and standard deviation for numeric variables such as popularity, vote_average, and vote_count. In addition, frequency distributions were conducted for categorical variables.\nData Visualization: Through histograms, box plots, and scatter plots, we conducted in-depth explorations of the distributions and relationships between features in the dataset. For example, the distribution of popularity shows that most of the movies in the dataset are moderately popular, but a few are very popular.\nCORRELATION ANALYSIS: We used correlation matrix and heatmap to check the relationship between the variables. From the results, we observed a significant positive correlation between popularity and vote_count, implying that movies with more ratings are usually more popular as well.\nOutlier Detection: Through the IQR method, we identified outliers in vote_average and several other variables. These outliers may indicate data quality issues or interesting anomalies in the dataset.\n3. Charts and tables.\nWe utilized a variety of charts and table formats to more visually display data distributions, trends, and relationships between features. For example, a scatterplot shows the trend of increasing popularity with increasing vote_count.\n4 Conclusions and insights.\nSummarizing the above analysis, we can draw the following conclusions and insights:\nThere is a strong relationship between the popularity of a movie and the number of votes it receives.\nThere are some outliers in the dataset, which may be due to data entry errors, or a true reflection of some specific situations.\nBy grouping and segmenting the data, we can gain more insight into trends and patterns within specific subgroups.\nFor further data analysis or modeling, we recommend more in-depth feature engineering, as well as considering the use of more advanced statistical models or machine learning techniques. In addition, based on our findings, producers or distributors could consider different promotional strategies to more effectively increase the visibility and popularity of their movies.\nAnswer for 9\nTools and Software.\nDuring Exploratory Data Analysis (EDA), choosing the right tools and software is critical as they help us to effectively process data, create visualizations, and draw meaningful conclusions. Below are the main tools and software we used for this project:\nR: This EDA was mainly analyzed using R. R is an open source programming language and software environment developed for statistical computing and graphical design. Thanks to its powerful statistical functions and plotting libraries, R is widely popular in the field of data analysis.\nggplot2: This is an R-based data visualization package that provides an efficient way to create complex statistical graphs. In this project, we used ggplot2 to create histograms, box plots, scatter plots, and other graphs to explore the characteristics of the dataset.\nOther R libraries: In addition to ggplot2, we used several other R libraries to help with EDA. e.g., the dplyr library for data manipulation and transformations, the tidyr library for data collation, and the correlation analysis using the corrplot library.\nThe choice of R and its associated libraries as the primary EDA tool was based on the following considerations:\nFlexibility: R offers a wide range of statistical and data analysis capabilities to meet a variety of complex analysis needs.\nCommunity Support: R has a very active community where users can find a large number of online tutorials, guides and answers to questions.\nExtensibility: R has thousands of available packages that can be used for a variety of statistical, machine learning, and data visualization tasks."
  },
  {
    "objectID": "eda/eda_cleaned_data_r1.html",
    "href": "eda/eda_cleaned_data_r1.html",
    "title": "Untitled",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# data &lt;- read.csv(\"../../.././data./01-modified-data./cleaned_data_r1.csv\", stringsAsFactors = FALSE)\ndata &lt;- read.csv(\"../../../data/01-modified-data/cleaned_data_r1.csv\", stringsAsFactors = FALSE)\n\n\nhead(data)\n\n           title score                           term\n1   肖申克的救赎   9.7                 希望让人自由。\n2       霸王别姬   9.6                     风华绝代。\n3       阿甘正传   9.5             一部美国近现代史。\n4     泰坦尼克号   9.5            失去的才是永恒的。 \n5 这个杀手不太冷   9.4 怪蜀黍和小萝莉不得不说的故事。\n6       千与千寻   9.4  最好的宫崎骏，最好的久石让。 \n                                  other                          quote\n1          月黑高飞(港)    刺激1995(台)                 希望让人自由。\n2 再见，我的妾    Farewell My Concubine                     风华绝代。\n3                         福雷斯特·冈普             一部美国近现代史。\n4                      铁达尼号(港  台)             失去的才是永恒的。\n5            终极追杀令(台)    杀手莱昂 怪蜀黍和小萝莉不得不说的故事。\n6        神隐少女(台)    千与千寻的神隐   最好的宫崎骏，最好的久石让。\n\n\n\nstr(data)\n\n'data.frame':   100 obs. of  5 variables:\n $ title: chr  \"肖申克的救赎\" \"霸王别姬\" \"阿甘正传\" \"泰坦尼克号\" ...\n $ score: num  9.7 9.6 9.5 9.5 9.4 9.4 9.6 9.4 9.6 9.4 ...\n $ term : chr  \"希望让人自由。\" \"风华绝代。\" \"一部美国近现代史。\" \"失去的才是永恒的。 \" ...\n $ other: chr  \"月黑高飞(港)    刺激1995(台)\" \"再见，我的妾    Farewell My Concubine\" \"福雷斯特·冈普\" \"铁达尼号(港  台)\" ...\n $ quote: chr  \"希望让人自由。\" \"风华绝代。\" \"一部美国近现代史。\" \"失去的才是永恒的。\" ...\n\n\n\nsummary(data$score)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  8.700   9.000   9.150   9.153   9.300   9.700 \n\n\n\nlength(unique(data$title)) == nrow(data)\n\n[1] TRUE\n\n\n\ncolSums(is.na(data))\n\ntitle score  term other quote \n    0     0     0     0     0 \n\n\n\nmean_score &lt;- mean(data$score, na.rm = TRUE)\nmedian_score &lt;- median(data$score, na.rm = TRUE)\nsd_score &lt;- sd(data$score, na.rm = TRUE)\nvar_score &lt;- var(data$score, na.rm = TRUE)\nget_mode &lt;- function(v) {\n  uniqv &lt;- unique(v)\n  uniqv[which.max(tabulate(match(v, uniqv)))]\n}\nmode_score &lt;- get_mode(data$score)\n\ncat(\"Mean score:\", mean_score, \"\\n\")\n\nMean score: 9.153 \n\ncat(\"Median score:\", median_score, \"\\n\")\n\nMedian score: 9.15 \n\ncat(\"Mode score:\", mode_score, \"\\n\")\n\nMode score: 9.1 \n\ncat(\"Standard Deviation of score:\", sd_score, \"\\n\")\n\nStandard Deviation of score: 0.2110364 \n\ncat(\"Variance of score:\", var_score, \"\\n\")\n\nVariance of score: 0.04453636 \n\n\n\nterm_freq &lt;- table(data$term)\nquote_freq &lt;- table(data$quote)\n\n#print(term_freq)\n#print(quote_freq)\n#too long\n\n\nlibrary(ggplot2)\n\nggplot(data, aes(x=term)) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title=\"Frequency Distribution of Terms\", x=\"Terms\", y=\"Frequency\")\n\n\n\nggplot(data, aes(x=quote)) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title=\"Frequency Distribution of Quotes\", x=\"Quotes\", y=\"Frequency\")\n\n\n\n\n\nggplot(data, aes(x=score)) +\n  geom_histogram(binwidth=0.05, fill=\"blue\", alpha=0.7) +\n  labs(title=\"Histogram of Scores\", x=\"Score\", y=\"Frequency\")\n\n\n\nggplot(data, aes(y=score)) +\n  geom_boxplot(fill=\"yellow\", outlier.color=\"red\", outlier.shape=21) +\n  labs(title=\"Boxplot of Scores\", y=\"Score\")\n\n\n\nggplot(data, aes(x=score)) +\n  geom_density(fill=\"green\", alpha=0.7) +\n  labs(title=\"Density Plot of Scores\", x=\"Score\", y=\"Density\")\n\n\n\n\nThere is only one int variable in this dataset, the rest are str variables, so you can’t do a correlation analysis.\nThis dataset is the top 100 ratings on a specialized movie website in China.\n1. Audience Preference for High Quality Movies: Since this is the data of the top 100 movie ratings and the scores are mainly concentrated in the range of 9 to 10, it may indicate that the audience prefers high quality movies on this website.\n2. Professional rating standard: Considering that the scores of most movies are concentrated in a small range, the rating standard of this movie website may be relatively professional and strict.\n3. Possible differences in viewers’ rating habits: Even though all these movies are in the top 100, differences in ratings between them still exist. This may mean that viewers have different rating habits, expectations and preferences.\n4. Classics vs. new releases: In the top one hundred, there may be some old classics while some are new releases in recent years. There may be differences in the distribution of ratings between them, which could be an interesting point of hypothesis.\n5. Differences in movie genres: Different types of movies (e.g., action, romance, sci-fi, etc.) may have differences in ratings. One hypothesis is that certain types of movies may be more popular or score higher on this site.\n6. Influence of directors and actors: Some directors or actors may score their movies higher due to their popularity and reputation in the industry. Audiences may be more inclined to give higher scores to movies with well-known directors or actors.\nThis dataset is mainly ratings and short reviews, there is no other valuable information, I will try to add these data manually later, here is a list of my assumptions first\n\ndata$group &lt;- cut(data$score, breaks = c(0, 9, 9.5, 10), labels = c(\"&lt;9.0\", \"9.0-9.5\", \"&gt;=9.5\"), include.lowest = TRUE, right = FALSE)\n\ngrouped_data &lt;- data %&gt;% group_by(group) %&gt;%\n  summarise(\n    count = n(),\n    avg_score = mean(score),\n    min_score = min(score),\n    max_score = max(score)\n  )\n\nprint(grouped_data)\n\n# A tibble: 3 × 5\n  group   count avg_score min_score max_score\n  &lt;fct&gt;   &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 &lt;9.0       21      8.86       8.7       8.9\n2 9.0-9.5    72      9.20       9         9.4\n3 &gt;=9.5       7      9.59       9.5       9.7\n\nggplot(grouped_data, aes(x = group, y = count)) +\n  geom_bar(stat=\"identity\", fill = \"steelblue\") +\n  labs(x = \"score group\", y = \"frequency\") +\n  theme_minimal()\n\n\n\n\n\nggplot(data, aes(x = score, fill = group)) +\n  geom_histogram(binwidth = 0.1, position=\"identity\", alpha=0.7) +\n  labs(title = \"score group distribution\", x = \"score\", y = \"frequency\") +\n  theme_minimal() +\n  facet_wrap(~group, scales = \"free_y\")\n\n\n\n\n\nlibrary(jiebaR)\n\nLoading required package: jiebaRD\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n✔ readr     2.1.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\njieba_cut &lt;- worker(bylines = F)  # 初始化分词函数\n\ndata$words &lt;- lapply(data$term, function(sentence) {\n  unlist(segment(sentence, jieba_cut))\n})\n\n\nstop_words &lt;- c(\"我\", \"你\", \"他\", \"她\", \"它\", \"我们\", \"你们\", \"他们\", \"的\", \"和\", \"在\", \n                \"是\", \"了\", \"有\", \"就\", \"可以\", \"都\", \"不\", \"一个\", \"为\", \"上\", \"和\", \n                \"也\", \"中\", \"到\", \"说\", \"要\", \"以\", \"这\", \"对\", \"与\", \"及\", \"很\", \"但\", \n                \"与其\", \"之\", \"等\", \"或\", \"如\", \"那\", \"而\", \"被\", \"人\", \"之一\", \"吧\", \"呢\", \n                \"吗\", \"会\", \"更\", \"还\", \"让\", \"做\", \"因\", \"所\", \"其\", \"没\", \"最\", \"自己\", \"之间\", \n                \"没有\", \"时\", \"年\", \"着\", \"之后\", \"应\", \"地\", \"得\", \"过\", \"大\", \"只\", \"等等\",\"的\")\n\ndata$words &lt;- lapply(data$term, function(sentence) {\n  words &lt;- unlist(segment(sentence, jieba_cut))\n  words[!words %in% stop_words]\n})\n\nword_count &lt;- data %&gt;% \n  unnest(words) %&gt;% \n  count(words, sort = TRUE) %&gt;%\n  filter(!words %in% stop_words)\nword_count\n\n# A tibble: 419 × 2\n   words     n\n   &lt;chr&gt; &lt;int&gt;\n 1 电影      7\n 2 不是      5\n 3 世界      5\n 4 人生      5\n 5 永远      5\n 6 一部      4\n 7 就是      4\n 8 才        4\n 9 故事      4\n10 这样      4\n# ℹ 409 more rows\n\n\n\nggplot(word_count[1:20,], aes(x = reorder(words, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"key word\", y = \"frequency\") +\n  theme_minimal()\n\n\n\n\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(jiebaR)\nlibrary(tm)\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\njiebar &lt;- worker()\n\nseg_words &lt;- unlist(lapply(data$term, function(x) segment(x, jiebar = jiebar)))\n\nseg_words_clean &lt;- seg_words[!seg_words %in% stop_words]\n\nwordcloud(words = seg_words_clean, min.freq = 2, max.words = 100, \n          random.order = FALSE, colors = brewer.pal(8, \"Dark2\"))\n\nWarning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation\ndrops documents\n\n\nWarning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x,\ntm::stopwords())): transformation drops documents\n\n\n\n\n\n\nlibrary(jiebaR)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\n\ngroup1 &lt;- data[data$score &gt;= 9.5,]\ngroup2 &lt;- data[data$score &lt; 9.5 & data$score &gt;= 9.0,]\ngroup3 &lt;- data[data$score &lt; 9.0,]\n\n\ngenerate_wordcloud &lt;- function(data_group) {\n  seg_words &lt;- unlist(lapply(data_group$term, function(x) segment(x, jiebar = jiebar)))\n  seg_words_clean &lt;- seg_words[!seg_words %in% stop_words]\n  wordcloud(words = seg_words_clean, min.freq = 2, max.words = 100, random.order = FALSE, colors = brewer.pal(8, \"Dark2\"))\n}\n\n\ngenerate_wordcloud(group1)  \n\nWarning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation\ndrops documents\n\n\nWarning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x,\ntm::stopwords())): transformation drops documents\n\n\n\n\ngenerate_wordcloud(group2)  \n\nWarning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation\ndrops documents\n\nWarning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation\ndrops documents\n\n\n\n\ngenerate_wordcloud(group3)  \n\nWarning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation\ndrops documents\n\nWarning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation\ndrops documents\n\n\n\n\n\n\nextract_keywords &lt;- function(data_group) {\n  seg_words &lt;- unlist(lapply(data_group$term, function(x) segment(x, jiebar = jiebar)))\n  seg_words_clean &lt;- seg_words[!seg_words %in% stop_words]\n  freq_table &lt;- table(seg_words_clean)\n  return(freq_table)\n}\nfreq_group1 &lt;- extract_keywords(group1) \nfreq_group2 &lt;- extract_keywords(group2) \nfreq_group3 &lt;- extract_keywords(group3)\ndf_group1 &lt;- as.data.frame(freq_group1) %&gt;% arrange(-Freq) %&gt;% head(20) # \ndf_group2 &lt;- as.data.frame(freq_group2) %&gt;% arrange(-Freq) %&gt;% head(20)\ndf_group3 &lt;- as.data.frame(freq_group3) %&gt;% arrange(-Freq) %&gt;% head(20)\n\nplot_keywords &lt;- function(df, title) {\n  ggplot(df, aes(x = reorder(seg_words_clean, Freq), y = Freq)) +\n    geom_bar(stat = \"identity\", fill = \"steelblue\") +\n    coord_flip() +\n    labs(title = title, x = \"Keywords\", y = \"Frequency\") +\n    theme_minimal()\n}\n\nplot_keywords(df_group1, \"Keywords for Group 1 (score &gt; 9.5)\")\n\n\n\nplot_keywords(df_group2, \"Keywords for Group 2 (9.0 &lt;= score &lt;= 9.5)\")\n\n\n\nplot_keywords(df_group3, \"Keywords for Group 3 (score &lt; 9.0)\")\n\n\n\n\n7.There is no identifying outliers.\nANSWER for 8\n\nIntroduction:\nThe purpose of this report is to provide an overview of the Exploratory Data Analysis (EDA) conducted on the cleaned_data_r1.csv dataset. The dataset primarily contains terms and their associated scores, offering insights into sentiments or importance. We delved into the data, uncovering patterns, distributions, and relationships that can help in further research or decision-making.\n\n\n1. Data Overview:\nThe dataset comprises terms (or phrases) and their respective scores. A preliminary assessment revealed that the data contains unique terms without any repetition.\n\n\n2. Data Distribution:\n\nScore Distribution: Most of the scores lie in the range between 9 and 10. The data indicates a high degree of positivity or favorability in terms.\n\n\n\n3. Frequency Analysis:\n\nKeyword Analysis: A frequency analysis of keywords revealed the most common themes in the terms. Some top keywords included words like “自由” (freedom), suggesting a possible focus on liberty or independence in the context from which this data has been extracted.\n\n\n\n4. Grouped Analysis:\nBased on the scores, we classified the terms into three main groups:\n\nGroup 1: Score &gt; 9.5\nGroup 2: 9.0 &lt;= Score &lt;= 9.5\nGroup 3: Score &lt; 9.0\n\nA keyword frequency analysis within these groups revealed nuanced insights:\n\nGroup 1 primarily focused on [Top 3 Keywords for Group 1].\nGroup 2 had a concentration on [Top 3 Keywords for Group 2].\nGroup 3 emphasized on [Top 3 Keywords for Group 3].\n\n(Note: Replace the placeholders [Top 3 Keywords for Group X] with actual keywords from the earlier analysis.)\n\n\n5. Visual Representation:\n\nHistogram: A histogram showcased the distribution of scores, indicating the concentration of terms in the higher score range.\nWord Cloud: A word cloud visualization highlighted the prominence of certain keywords in the dataset, with size indicating frequency.\n\n\n\n6. Potential Areas of Interest:\n\nCertain keywords like “自由” (freedom) appear prominently across groups, indicating their universal appeal or significance.\nThe majority of terms have a score above 9.0, pointing towards a generally favorable or positive sentiment.\n\n\n\nConclusion:\nThe EDA on cleaned_data_r1.csv has unveiled several insights into the nature and themes of the terms. The data leans towards positivity, with particular emphasis on certain key themes. These insights can provide a foundational understanding for further in-depth analysis, modeling, or strategic decision-making based on the dataset’s context.\n\nNote: The above report provides a structured summary based on the EDA activities we’ve conducted thus far. It’s essential to adjust or expand upon these findings if more in-depth analysis or additional insights were gleaned from the data.\nANSWER for 9\nTools and Software for Exploratory Data Analysis (EDA) on “cleaned_data_r1.csv”\nIn the process of handling and analyzing the dataset “cleaned_data_r1.csv,” various tools and software were utilized. Here’s a tailored discussion on those tools and their specific roles:\nR:\nGiven that the operations performed on “cleaned_data_r1.csv” were conducted in R, it stands as the primary language for this EDA.\nggplot2: For creating visualizations such as bar plots, histograms, and scatter plots, ggplot2 was instrumental. Its flexibility and layering principle allowed for custom-tailored visualizations, highlighting specific patterns and insights from the data.\ndplyr: This package was frequently employed for data manipulation tasks, including filtering, grouping, and summarizing the data. Its intuitive syntax made these operations efficient.\ntidyr: Whenever the data needed reshaping or restructuring, tidyr provided the necessary functions. This was especially useful in ensuring the dataset adhered to the tidy data principles.\nwordcloud\nFor visual representation of textual data and to highlight frequently occurring terms, the wordcloud2 package in R was used. This visual tool allowed for quick identification of patterns in textual data, especially when analyzing terms or phrases from the dataset.\nJiebaR:\nGiven the presence of Chinese text in the dataset, the JiebaR package played a crucial role. It was used for segmenting Chinese text into words, facilitating term frequency analysis and ensuring the word clouds generated were accurate and insightful.\nIn Conclusion:\nThe combination of R with its diverse packages and the RStudio environment ensured a comprehensive and insightful EDA on “cleaned_data_r1.csv.” These tools, combined with domain knowledge and analytical techniques, were key in extracting meaningful insights from the data."
  },
  {
    "objectID": "Naive_Bayes_text_data.html",
    "href": "Naive_Bayes_text_data.html",
    "title": "Naïve Bayes (NB) with Labeled Text Data",
    "section": "",
    "text": "1. Read text file\n\nimport pandas as pd\n\nfile_name = '../../data/others/IMDB-Dataset.csv'\ndf=pd.read_csv(file_name)  \nprint(df.shape)\n\n(50000, 2)\n\n\n2. Preprocess the data: convert from string labels to integers\n\nimport numpy as np\n \nlabels=[]; \nsentiment=[]\nfor label in df[\"sentiment\"]:\n    if label not in labels:\n        labels.append(label)\n        print(\"index =\",len(labels)-1,\": label =\",label)\n    for i in range(0,len(labels)):\n        if(label==labels[i]):\n            sentiment.append(i)\nsentiment=np.array(sentiment)\n \ncorpus=df[\"review\"].to_list()\nprint(\"number of text chunks = \",len(corpus))\nprint(corpus[0:3])\n\nindex = 0 : label = positive\nindex = 1 : label = negative\nnumber of text chunks =  50000\n[\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.&lt;br /&gt;&lt;br /&gt;It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.&lt;br /&gt;&lt;br /&gt;I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\", 'A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. &lt;br /&gt;&lt;br /&gt;The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.', 'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.&lt;br /&gt;&lt;br /&gt;This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.&lt;br /&gt;&lt;br /&gt;This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.']\n\n\n3. Vectorize reviews\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer=CountVectorizer(min_df=0.001)   \nXs  =  vectorizer.fit_transform(corpus)   \nX=np.array(Xs.todense())\nmaxs=np.max(X,axis=0)\nX=np.ceil(X/maxs)\nprint(X.shape)\nprint(sentiment.shape)\n\n(50000, 10190)\n(50000,)\n\n\n4. Split data into train and test part\n\nfrom sklearn.model_selection import train_test_split\ny= sentiment\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=100\n)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n(40000, 10190) (10000, 10190) (40000,) (10000,)\n\n\n5. Naive Bayes Model start fitting\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n\nmodel.score(X_train, y_train)\n\n0.8611\n\n\nHere the trained model is tested on the testing dataset.\n\ny_pred = model.predict(X_test)\n\n6. Evaluation\nAccuracy: Measures the correct prediction ratio. Effective for balanced classes, less so for imbalanced ones. Precision: Proportion of true positives in positive predictions. Key when false positives have high costs. Recall (Sensitivity): Proportion of actual positives correctly identified. Critical when false negatives carry significant risks. F1 Score: Harmonic mean of precision and recall. Ideal for balancing these metrics, particularly in imbalanced datasets.\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\naccuracy=accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\nprecision = precision_score(y_test, y_pred, average=\"weighted\")\nrecall = recall_score(y_test, y_pred, average=\"weighted\")\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\n\ncm=confusion_matrix(y_test, y_pred)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"positive\",\"negative\"])\ndisp.plot()\n\nAccuracy: 0.8584\nPrecision: 0.8589150250548824\nRecall: 0.8584\nF1 Score: 0.8584024638424639\n\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1071c9880&gt;\n\n\n\n\n\nThe model’s performance - Accuracy (0.8584): Quite high, which means that among all predictions, the majority are correct - Precision (0.8589150250548824): When the model predicts that a certain observation value is positive, there is an 85.89% probability that it is correct. Indicates good reliability. - Recall (0.8584): Identifies 85.8% of actual positives. High performance in detecting true cases. - F1 Score (0.8584024638424639): High mean of precision and recall, indicating high balance.\nOverfitting or underfitting The definition for each term could be found in the Part for Naïve Bayes (NB) with Labeled Record Data. From the output from the above code, we found that the trained model performance on trained data is 0.8611, which is basiclly similar to the performance of trained model on testing data. Thus, the model is neither overfitting nor underfitting.\nConclusion The metrics indicate that the model performs well in overall performance, with relatively high accuracy, precision, recall, and F1 score. This means that the model performs evenly when dealing with positive and negative class samples, without a clear bias towards over identifying a particular class. Overall, the sentiment is highly related to the moview review."
  },
  {
    "objectID": "dimensionality_reduction.html",
    "href": "dimensionality_reduction.html",
    "title": "Project Proposal:",
    "section": "",
    "text": "Project’s objectives\nDataset selection Here I select the clean data for the movie data\nThe tools or libraries The code will be implemented in Python. The library pandas and numpy will be used to read and process the data. The library scikit-learn will be used to implement the model."
  },
  {
    "objectID": "dimensionality_reduction.html#project-proposal",
    "href": "dimensionality_reduction.html#project-proposal",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "A brief proposal outlining your project’s objectives, dataset selection, and the tools or libraries you plan to use (e.g., Python, scikit-learn)."
  },
  {
    "objectID": "dimensionality_reduction.html#code-implementation",
    "href": "dimensionality_reduction.html#code-implementation",
    "title": "Project Proposal:",
    "section": "Code Implementation:",
    "text": "Code Implementation:\nRead file\n\nimport pandas as pd\n\nfile_name = '../../data/others/IMDB_Top_250_Movies_ver2.csv'\ndf=pd.read_csv(file_name)  \ndf = df.drop(['Unnamed: 0'], axis=1)\nprint(df.shape)\n\n(250, 5)\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nyear\nrating\ngenre\ncasts\ndirectors\n\n\n\n\n0\n1994\n9.3\nDrama\nTim Robbins,Morgan Freeman,Bob Gunton\nFrank Darabont\n\n\n1\n1972\n9.2\nCrime,Drama\nMarlon Brando,Al Pacino,James Caan\nFrancis Ford Coppola\n\n\n2\n2008\n9.0\nAction,Crime,Drama\nChristian Bale,Heath Ledger,Aaron Eckhart\nChristopher Nolan\n\n\n3\n1974\n9.0\nCrime,Drama\nAl Pacino,Robert De Niro,Robert Duvall\nFrancis Ford Coppola\n\n\n4\n1957\n9.0\nCrime,Drama\nHenry Fonda,Lee J. Cobb,Martin Balsam\nSidney Lumet\n\n\n\n\n\n\n\nPreprocess the data\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# genre\nmlb = MultiLabelBinarizer()\ndf_genres = mlb.fit_transform(df['genre'])\ndf_genres = pd.DataFrame(df_genres, columns=mlb.classes_)\ndf = df.drop('genre', axis=1).join(df_genres)\n\n#casts\ndf['casts'] = df['casts'].apply(lambda x: x.split(','))\ndf_casts = mlb.fit_transform(df['casts'])\ndf_casts = pd.DataFrame(df_casts, columns=mlb.classes_)\ndf = df.drop('casts', axis=1).join(df_casts)\n\n# certificate\n# encoder = OneHotEncoder(sparse=False)\n# ct = ColumnTransformer([(\"certificate\", encoder, ['certificate'])], remainder='passthrough')\n# df_encoded = ct.fit_transform(df)\n# df_encoded = pd.DataFrame(df_encoded, columns=ct.get_feature_names_out())\n# df = df.drop('certificate', axis=1).join(df_encoded)\n\n# directors\nencoder = OneHotEncoder(sparse=False)\nct = ColumnTransformer([(\"directors\", encoder, ['directors'])], remainder='passthrough')\ndf_encoded = ct.fit_transform(df)\ndf_encoded = pd.DataFrame(df_encoded, columns=ct.get_feature_names_out())\ndf = df.drop('directors', axis=1)\ndf = pd.concat([df, df_encoded], axis=1)\n\n\nprint(df.head())\n\nX = np.array(df)\n\n   year  rating  ,  -  A  B  C  D  F  H  ...  remainder__Yuriko Ishida  \\\n0  1994     9.3  0  0  0  0  0  1  0  0  ...                       0.0   \n1  1972     9.2  1  0  0  0  1  1  0  0  ...                       0.0   \n2  2008     9.0  1  0  1  0  1  1  0  0  ...                       0.0   \n3  1974     9.0  1  0  0  0  1  1  0  0  ...                       0.0   \n4  1957     9.0  1  0  0  0  1  1  0  0  ...                       0.0   \n\n   remainder__Yuriy Solomin  remainder__Yutaka Sada  remainder__Yves Montand  \\\n0                       0.0                     0.0                      0.0   \n1                       0.0                     0.0                      0.0   \n2                       0.0                     0.0                      0.0   \n3                       0.0                     0.0                      0.0   \n4                       0.0                     0.0                      0.0   \n\n   remainder__Yôji Matsuda  remainder__Yûko Tanaka  remainder__Zain Al Rafeea  \\\n0                      0.0                     0.0                        0.0   \n1                      0.0                     0.0                        0.0   \n2                      0.0                     0.0                        0.0   \n3                      0.0                     0.0                        0.0   \n4                      0.0                     0.0                        0.0   \n\n   remainder__Zazie Beetz  remainder__Zendaya  remainder__Çetin Tekindor  \n0                     0.0                 0.0                        0.0  \n1                     0.0                 0.0                        0.0  \n2                     0.0                 0.0                        0.0  \n3                     0.0                 0.0                        0.0  \n4                     0.0                 0.0                        0.0  \n\n[5 rows x 1404 columns]\n\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning:\n\n`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n\n\n\nDimensionality Reduction with PCA 1. Apply PCA to record dataset.\n\nfrom sklearn.decomposition import PCA\n\npca_original = PCA()\npca_original.fit(X)\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA()\n\n\n\nDetermine the optimal number of principal components to retain.\n\n\nimport matplotlib.pyplot as plt\n\ncumulative_variance = np.cumsum(pca_original.explained_variance_ratio_)\nn_components = np.argmax(cumulative_variance &gt;= 0.8) + 1\n\nplt.figure(figsize=(8, 4))\nplt.plot(np.cumsum(pca_original.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Explained Variance by Components')\nplt.grid(True)\nplt.show()\n\n\n\n\n\nVisualize the reduced-dimensional data using PCA.\n\n\nimport seaborn as sns\n\npca = PCA(n_components=n_components)\npca.fit(X)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\n\nprincipal_components = pca.fit_transform(X)\ndf2 = pd.DataFrame(data = principal_components)\ndf3=pd.concat([df2,df['rating']], axis=1)\n\nsns.scatterplot(data=df2, x=0, y=1,hue=df[\"rating\"]) \nplt.show()\n\nsns.pairplot(data=df3,hue=\"rating\") \nplt.show()\n\n[0.98686378]\n[560.7301677]\n\n\n\n\n\n\n\n\nDimensionality Reduction with t-SNE 1. Implement t-SNE on the same dataset.\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndata_standardized = scaler.fit_transform(X)\n\nperplexities = [5, 30, 50, 100] \ntsne_results = []\n\nfor perplexity in perplexities:\n    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=0)\n    tsne_result = tsne.fit_transform(data_standardized)\n    tsne_results.append(tsne_result)\n\n \nfig, axes = plt.subplots(1, len(perplexities) + 1, figsize=(15, 7))\nfor i, tsne_result in enumerate(tsne_results):\n    axes[i + 1].scatter(tsne_result[:, 0], tsne_result[:, 1], c=df[\"rating\"])\n    axes[i + 1].set_title(f't-SNE with Perplexity = {perplexities[i]}')\nplt.show()\n\n\n\n\n\nExplore different perplexity values and their impact.\n\nSummay Compare the visualization, PCA is more intuitive in this case\nIn terms of the trade-offs and scenarios where one technique may outperform the other, here’s the summary:\nPCA is a linear algorithm. The principal components are the linear combination of the input variables, thus it is more interpretable. Moreover, it is effective when you need preserve the global structure of the data. Therefore, when reducing the dimension of data, the dimensions with low variance will often be discarded, and potentially losing important information of data.\nt-SNE is a non-linear algorithm. Thus it can visualize complex data better than PCA. It is effective when you need preserve the local structure of the data. Therefore, it may not preserve the glocal feature of the data. However, t-SNE costs more computational resources, especially for large datasets."
  },
  {
    "objectID": "dimensionality_reduction.html#project-report",
    "href": "dimensionality_reduction.html#project-report",
    "title": "Project Proposal:",
    "section": "Project Report:",
    "text": "Project Report:\nA comprehensive project report detailing the steps taken, results obtained, and your analysis. Include visualizations, comparisons, and insights gained from the dimensionality reduction techniques."
  },
  {
    "objectID": "clustering/clustering.html#kmean",
    "href": "clustering/clustering.html#kmean",
    "title": "Clustering",
    "section": "KMEAN",
    "text": "KMEAN\nKMEAN is a machine learning method for clustering. It can group the similar data together by iteration. The idea of the K-Means algorithm is very simple. For a given sample set, the sample set is divided into K clusters based on the distance between the samples. The algorithm will make the points within the cluster as closely connected as possible, while keeping the distance between clusters as large as possible. The Elbow method iteratively calculates k from 1 to n for a dataset of n points. After each clustering is completed, the sum of squares of the distance between each point and the center of the cluster it belongs to is calculated. It can be imagined that this sum of squares will gradually decrease until k=n, when the sum of squares is 0, because each point is the center of the cluster it belongs to."
  },
  {
    "objectID": "clustering/clustering.html#dbsan",
    "href": "clustering/clustering.html#dbsan",
    "title": "Clustering",
    "section": "DBSAN",
    "text": "DBSAN\nDBSCAN is a clustering algorithm based on density. It clusters points that are very close to each other into a class, and also marks points in low-density areas as outliers. Firstly, the algorithm uses the specified neighborhood density parameter to find the core points among all points and determine the set of core points. Then it randomly select a core point from the set, and find all samples that can reach its density to generate clustering clusters. While iteration, the remaining core points that have not been clustered in the set will be randomly selected. Until all clusters with achievable density of core points are completely discovered, the iteration will be stopped."
  },
  {
    "objectID": "clustering/clustering.html#hierachical-clustering-1",
    "href": "clustering/clustering.html#hierachical-clustering-1",
    "title": "Clustering",
    "section": "Hierachical Clustering",
    "text": "Hierachical Clustering\n\ndistance_metrics = ['euclidean', 'cityblock', 'cosine']\n\nplt.figure(figsize=(15, 5))\n\nfor i, metric in enumerate(distance_metrics, 1):\n    if metric != 'cosine':\n        Y = pdist(X, metric=metric)\n\n    else:\n        Y = pdist(X, lambda u, v: cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0])\n        Y = 1 - Y  \n\n \n    Z = linkage(squareform(Y), method='average') \n    plt.subplot(1, 3, i)\n    dendrogram(Z)\n    plt.title(f\"Hierarchical Clustering with {metric} Distance\")\n    plt.xlabel('Sample Index')\n    plt.ylabel('Distance')\n\nplt.tight_layout()\nplt.show()\n\n/var/folders/g1/bhyt3dwj3119y7zfj69zl2mr0000gn/T/ipykernel_14699/3423943772.py:14: ClusterWarning:\n\nscipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n\n/var/folders/g1/bhyt3dwj3119y7zfj69zl2mr0000gn/T/ipykernel_14699/3423943772.py:14: ClusterWarning:\n\nscipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n\n/var/folders/g1/bhyt3dwj3119y7zfj69zl2mr0000gn/T/ipykernel_14699/3423943772.py:14: ClusterWarning:\n\nscipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix"
  },
  {
    "objectID": "clustering/clustering.html#hierachical-clustering",
    "href": "clustering/clustering.html#hierachical-clustering",
    "title": "Clustering",
    "section": "Hierachical clustering",
    "text": "Hierachical clustering\nThe hierarchical clustering algorithm divides the dataset into clusters layer by layer, and the clusters generated by the subsequent layer are based on the results of the previous layer. Hierarchical clustering algorithms are generally divided into two categories: Divisive hierarchical clustering: also known as top-down hierarchical clustering, where all objects initially belong to a cluster, and each time a cluster is divided into multiple clusters according to certain criteria, repeating this process until each object is a cluster. Agglomerative hierarchical clustering: also known as bottom-up hierarchical clustering, where each object starts as a cluster and merges the two closest clusters according to certain criteria to generate a new cluster. This process repeats until all objects belong to the same cluster."
  },
  {
    "objectID": "clustering/clustering.html#kmean-1",
    "href": "clustering/clustering.html#kmean-1",
    "title": "Clustering",
    "section": "KMEAN",
    "text": "KMEAN\n\nElbow method\n\n# Elbow method\ninertia = []\nfor n_clusters in range(1, 11):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n    inertia.append(kmeans.inertia_)\n\n \nplt.figure(figsize=(8, 4))\nplt.plot(range(1, 11), inertia, marker='o')\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\n\n\nSilhouette analysis\n\n# Silhouette analysis\nsilhouette_coefficients = []\nfor n_clusters in range(2, 11):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n    score = silhouette_score(X, kmeans.labels_)\n    silhouette_coefficients.append(score)\nplt.figure(figsize=(8, 4))\nplt.plot(range(2, 11), silhouette_coefficients, marker='o')\nplt.title('Silhouette Analysis')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Coefficient')\nplt.show()\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/homebrew/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning"
  },
  {
    "objectID": "clustering/clustering.html#dbsan-1",
    "href": "clustering/clustering.html#dbsan-1",
    "title": "Clustering",
    "section": "DBSAN",
    "text": "DBSAN\n\ndbscan = DBSCAN(eps=0.1, min_samples=10)\ndbscan.fit(X)\nlabels = dbscan.labels_\ncore_samples_mask = np.zeros_like(labels, dtype=bool)\ncore_samples_mask[dbscan.core_sample_indices_] = True\n\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n\nfor k, col in zip(unique_labels, colors):\n\n    if k == -1:\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6)\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=14)\n\n \nplt.title('Estimated number of clusters: %d' % len(unique_labels))\nplt.show()"
  }
]