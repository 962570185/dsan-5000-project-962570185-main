---
title: "Naïve Bayes (NB) with Labeled Text Data"
format:
  html:
    embed-resources: true
jupyter: python3
---

**1. Read text file**
```{python}
import pandas as pd

file_name = '../../data/others/IMDB-Dataset.csv'
df=pd.read_csv(file_name)  
print(df.shape)
```

**2. Preprocess the data: convert from string labels to integers**
```{python}
import numpy as np
 
labels=[]; 
sentiment=[]
for label in df["sentiment"]:
    if label not in labels:
        labels.append(label)
        print("index =",len(labels)-1,": label =",label)
    for i in range(0,len(labels)):
        if(label==labels[i]):
            sentiment.append(i)
sentiment=np.array(sentiment)
 
corpus=df["review"].to_list()
print("number of text chunks = ",len(corpus))
print(corpus[0:3])
```

**3. Vectorize reviews**
```{python}
from sklearn.feature_extraction.text import CountVectorizer

vectorizer=CountVectorizer(min_df=0.001)   
Xs  =  vectorizer.fit_transform(corpus)   
X=np.array(Xs.todense())
maxs=np.max(X,axis=0)
X=np.ceil(X/maxs)
print(X.shape)
print(sentiment.shape)
```

**4. Split data into train and test part**
```{python}
from sklearn.model_selection import train_test_split
y= sentiment

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=100
)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
```

**5. Naive Bayes**
**Model start fitting**
```{python}
from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X_train, y_train)

```

```{python}
model.score(X_train, y_train)
```

**Here the trained model is tested on the testing dataset.**
```{python}
y_pred = model.predict(X_test)
```

**6. Evaluation**
**Metrics**
**Accuracy**: Measures the correct prediction ratio. Effective for balanced classes, less so for imbalanced ones.
**Precision**: Proportion of true positives in positive predictions. Key when false positives have high costs.
**Recall (Sensitivity)**: Proportion of actual positives correctly identified. Critical when false negatives carry significant risks.
**F1 Score**: Harmonic mean of precision and recall. Ideal for balancing these metrics, particularly in imbalanced datasets.

```{python}
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

accuracy=accuracy_score(y_pred, y_test)
f1 = f1_score(y_pred, y_test, average="weighted")
precision = precision_score(y_test, y_pred, average="weighted")
recall = recall_score(y_test, y_pred, average="weighted")

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

cm=confusion_matrix(y_test, y_pred)
disp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["positive","negative"])
disp.plot()
```

**The model’s performance**
- **Accuracy (0.8584)**: Quite high, which means that among all predictions, the majority are correct
- **Precision (0.8589150250548824)**: When the model predicts that a certain observation value is positive, there is an 85.89% probability that it is correct. Indicates good reliability.
- **Recall (0.8584)**: Identifies 85.8% of actual positives. High performance in detecting true cases.
- **F1 Score (0.8584024638424639)**: High mean of precision and recall, indicating high balance.


**Overfitting or underfitting**
The definition for each term could be found in the Part for Naïve Bayes (NB) with Labeled Record Data.
From the output from the above code, we found that the trained model performance on trained data is 0.8611, which is basiclly similar to the performance of trained model on testing data. Thus, the model is neither overfitting nor underfitting.

**Conclusion**
The metrics indicate that the model performs well in overall performance, with relatively high accuracy, precision, recall, and F1 score. This means that the model performs evenly when dealing with positive and negative class samples, without a clear bias towards over identifying a particular class. Overall, the sentiment is highly related to the moview review.
