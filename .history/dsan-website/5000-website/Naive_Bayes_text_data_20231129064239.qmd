---
title: "Naïve Bayes (NB) with Labeled Text Data"
format:
  html:
    embed-resources: true
jupyter: python3
---

**1. Read text file**
```{python}
import pandas as pd

file_name = '../../data/others/IMDB-Dataset.csv'
df=pd.read_csv(file_name)  
print(df.shape)
```

**2. Preprocess the data: convert from string labels to integers**
```{python}
import numpy as np
 
labels=[]; 
sentiment=[]
for label in df["sentiment"]:
    if label not in labels:
        labels.append(label)
        print("index =",len(labels)-1,": label =",label)
    for i in range(0,len(labels)):
        if(label==labels[i]):
            sentiment.append(i)
sentiment=np.array(sentiment)
 
corpus=df["review"].to_list()
print("number of text chunks = ",len(corpus))
print(corpus[0:3])
```

**3. Vectorize reviews**
```{python}
from sklearn.feature_extraction.text import CountVectorizer

vectorizer=CountVectorizer(min_df=0.001)   
Xs  =  vectorizer.fit_transform(corpus)   
X=np.array(Xs.todense())
maxs=np.max(X,axis=0)
X=np.ceil(X/maxs)
print(X.shape)
print(sentiment.shape)
```

**4. Split data into train and test part**
```{python}
from sklearn.model_selection import train_test_split
y= sentiment

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=100
)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
```

**5. Naive Bayes**
**Model start fitting**
```{python}
from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X_train, y_train)

```

```{python}
model.score(X_train, y_train)
```

**Here the trained model is tested on the testing dataset.**
```{python}
y_pred = model.predict(X_test)
```

**6. Evaluation**
**Metrics**
**Accuracy**: Measures the correct prediction ratio. Effective for balanced classes, less so for imbalanced ones.
**Precision**: Proportion of true positives in positive predictions. Key when false positives have high costs.
**Recall (Sensitivity)**: Proportion of actual positives correctly identified. Critical when false negatives carry significant risks.
**F1 Score**: Harmonic mean of precision and recall. Ideal for balancing these metrics, particularly in imbalanced datasets.

```{python}
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

accuracy=accuracy_score(y_pred, y_test)
f1 = f1_score(y_pred, y_test, average="weighted")
precision = precision_score(y_test, y_pred, average="weighted")
recall = recall_score(y_test, y_pred, average="weighted")

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

cm=confusion_matrix(y_test, y_pred)
disp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["positive","negative"])
disp.plot()
```

**The model’s performance**
- **Accuracy (0.504)**: Marginally better than random guessing. Effective if baseline accuracy is low.
- **Precision (0.5039)**: Correct half the time on positive predictions. Indicates average reliability.
- **Recall (0.504)**: Identifies 50.4% of actual positives. Moderate performance in detecting true cases.
- **F1 Score (0.5042)**: Harmonic mean of precision and recall, indicating average balance.
**Conclusion**
**Overfitting or underfitting**
The definition for each term could be found in the Part for Naive



By analyzing the movie data using a plain Bayesian model, we obtained some interesting findings. First, the accuracy of the model may not be as high as we expect, suggesting that our data features may not fully capture all the important information that determines movie ratings. In addition, the heatmap of the confusion matrix shows that the model predicts certain ratings more accurately than others. The distribution plots of the features further reveal possible trends between specific ratings and certain features, which provides insight into the relationship between movie ratings and these features. While the performance of the model could be improved, this analysis provides us with a benchmark that could be considered in the future to further optimize model performance using more complex models or more features. Overall, this study provides an initial understanding of the relationship between movie ratings and their features and provides a solid foundation for future in-depth research.
