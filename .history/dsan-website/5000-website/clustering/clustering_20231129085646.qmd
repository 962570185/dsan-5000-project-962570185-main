---
title: "Clustering"
---

Build out your website tab for "clustering"

**Get data**
```{python}
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

file_name = '../../data/others/IMDB_Top_250_Movies_ver2.csv'
df=pd.read_csv(file_name)  
df = df.drop(['Unnamed: 0'], axis=1)
print(df.shape)

# genre
mlb = MultiLabelBinarizer()
df_genres = mlb.fit_transform(df['genre'])
df_genres = pd.DataFrame(df_genres, columns=mlb.classes_)
df = df.drop('genre', axis=1).join(df_genres)

#casts
df['casts'] = df['casts'].apply(lambda x: x.split(','))
df_casts = mlb.fit_transform(df['casts'])
df_casts = pd.DataFrame(df_casts, columns=mlb.classes_)
df = df.drop('casts', axis=1).join(df_casts)

# directors
encoder = OneHotEncoder(sparse=False)
ct = ColumnTransformer([("directors", encoder, ['directors'])], remainder='passthrough')
df_encoded = ct.fit_transform(df)
df_encoded = pd.DataFrame(df_encoded, columns=ct.get_feature_names_out())
df = df.drop('directors', axis=1)
df = pd.concat([df, df_encoded], axis=1)


print(df.head())

X = np.array(df)

```

**Theory**

# Theory

## KMEAN
KMEAN is a machine learning method for clustering. It can group the similar data together by iteration.
The idea of the K-Means algorithm is very simple. For a given sample set, the sample set is divided into K clusters based on the distance between the samples. The algorithm will make the points within the cluster as closely connected as possible, while keeping the distance between clusters as large as possible. The Elbow method iteratively calculates k from 1 to n for a dataset of n points. After each clustering is completed, the sum of squares of the distance between each point and the center of the cluster it belongs to is calculated. It can be imagined that this sum of squares will gradually decrease until k=n, when the sum of squares is 0, because each point is the center of the cluster it belongs to.

## DBSAN
DBSCAN is a clustering algorithm based on density. It clusters points that are very close to each other into a class, and also marks points in low-density areas as outliers. Firstly, the algorithm uses the specified neighborhood density parameter to find the core points among all points and determine the set of core points. Then it randomly select a core point from the set, and find all samples that can reach its density to generate clustering clusters. While iteration, the remaining core points that have not been clustered in the set will be randomly selected. Until all clusters with achievable density of core points are completely discovered, the iteration will be stopped.

# Hierachical clustering
The hierarchical clustering algorithm divides the dataset into clusters layer by layer, and the clusters generated by the subsequent layer are based on the results of the previous layer. Hierarchical clustering algorithms are generally divided into two categories:
Divisive hierarchical clustering: also known as top-down hierarchical clustering, where all objects initially belong to a cluster, and each time a cluster is divided into multiple clusters according to certain criteria, repeating this process until each object is a cluster.
Agglomerative hierarchical clustering: also known as bottom-up hierarchical clustering, where each object starts as a cluster and merges the two closest clusters according to certain criteria to generate a new cluster. This process repeats until all objects belong to the same cluster.

**Method**

```{python}
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import DBSCAN
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist, squareform
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

```

# KMEAN
### Elbow method
```{python}
# Elbow method
inertia = []
for n_clusters in range(1, 11):
    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)
    inertia.append(kmeans.inertia_)

 
plt.figure(figsize=(8, 4))
plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()
```

### Silhouette analysis
```{python}
# Silhouette analysis
silhouette_coefficients = []
for n_clusters in range(2, 11):
    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)
    score = silhouette_score(X, kmeans.labels_)
    silhouette_coefficients.append(score)
plt.figure(figsize=(8, 4))
plt.plot(range(2, 11), silhouette_coefficients, marker='o')
plt.title('Silhouette Analysis')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Coefficient')
plt.show()
```

# DBSAN
```{python}
dbscan = DBSCAN(eps=0.1, min_samples=10)
dbscan.fit(X)
labels = dbscan.labels_
core_samples_mask = np.zeros_like(labels, dtype=bool)
core_samples_mask[dbscan.core_sample_indices_] = True

unique_labels = set(labels)
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

for k, col in zip(unique_labels, colors):

    if k == -1:
        col = [0, 0, 0, 1]

    class_member_mask = (labels == k)

    xy = X[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6)

    xy = X[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=14)

 
plt.title('Estimated number of clusters: %d' % len(unique_labels))
plt.show()

```

## Hierachical Clustering

```{python}
distance_metrics = ['euclidean', 'cityblock', 'cosine']

plt.figure(figsize=(15, 5))

for i, metric in enumerate(distance_metrics, 1):
    if metric != 'cosine':
        Y = pdist(X, metric=metric)

    else:
        Y = pdist(X, lambda u, v: cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0])
        Y = 1 - Y  

 
    Z = linkage(squareform(Y), method='average') 
    plt.subplot(1, 3, i)
    dendrogram(Z)
    plt.title(f"Hierarchical Clustering with {metric} Distance")
    plt.xlabel('Sample Index')
    plt.ylabel('Distance')

plt.tight_layout()
plt.show()
```