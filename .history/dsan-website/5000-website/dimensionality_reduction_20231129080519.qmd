## Project Proposal:
1. Projectâ€™s objectives


2. Dataset selection
Here I select the clean data for the movie data

3. The tools or libraries
The code will be implemented in Python. The library pandas and numpy will be used to read and process the data. The library scikit-learn will be used to implement the model.

## Code Implementation:

**Read file**
```{python}
import pandas as pd

file_name = '../../data/others/IMDB_Top_250_Movies_ver2.csv'
df=pd.read_csv(file_name)  
df = df.drop(['Unnamed: 0'], axis=1)
print(df.shape)
```

```{python}
df.head()
```

**Preprocess the data**
```{python}
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import CountVectorizer

# genre
mlb = MultiLabelBinarizer()
df_genres = mlb.fit_transform(df['genre'])
df_genres = pd.DataFrame(df_genres, columns=mlb.classes_)
df = df.drop('genre', axis=1).join(df_genres)

#casts
df['casts'] = df['casts'].apply(lambda x: x.split(','))
df_casts = mlb.fit_transform(df['casts'])
df_casts = pd.DataFrame(df_casts, columns=mlb.classes_)
df = df.drop('casts', axis=1).join(df_casts)

# certificate
encoder = OneHotEncoder(sparse=False)
ct = ColumnTransformer([("certificate", encoder, ['certificate'])], remainder='passthrough')
df_encoded = ct.fit_transform(df)
df_encoded = pd.DataFrame(df_encoded, columns=ct.get_feature_names_out())

# directors
encoder = OneHotEncoder(sparse=False)
ct = ColumnTransformer([("directors", encoder, ['directors'])], remainder='passthrough')
df_encoded = ct.fit_transform(df)
df_encoded = pd.DataFrame(df_encoded, columns=ct.get_feature_names_out())

# name
corpus=df["name"].to_list()
print("number of text chunks = ",len(corpus))
print(corpus[0:3])
vectorizer=CountVectorizer(min_df=0.001)   
Xs  =  vectorizer.fit_transform(corpus)   
X=np.array(Xs.todense())
maxs=np.max(X,axis=0)
X=np.ceil(X/maxs)
print(X)

print(df.head())
```


**Dimensionality Reduction with PCA**
1. Apply PCA to record dataset.
```{python}
from sklearn.decomposition import PCA

pca_original = PCA()
pca_original.fit(X)
```

2. Determine the optimal number of principal components to retain.
```{python}
cumulative_variance = np.cumsum(pca_original.explained_variance_ratio_)
n_components = np.argmax(cumulative_variance >= 0.8) + 1
```

3. Visualize the reduced-dimensional data using PCA.
```{python}
pca = PCA(n_components=n_components)
pca.fit(X)
print(pca.explained_variance_ratio_)
print(pca.singular_values_)

principal_components = pca.fit_transform(X)
df2 = pd.DataFrame(data = principal_components)
df3=pd.concat([df2,df['sentiment']], axis=1)

sns.scatterplot(data=df2, x=0, y=1,hue=df["sentiment"]) 
plt.show()

sns.pairplot(data=df3,hue="sentiment") 
plt.show()
```


**Dimensionality Reduction with t-SNE**
Implement t-SNE on the same dataset.
Explore different perplexity values and their impact.
Visualize the t-SNE output to reveal patterns and clusters.
Compare t-SNE results with PCA results.

Evaluation and Comparison
Evaluate the effectiveness of PCA and t-SNE in terms of preserving data structure and information.
Compare the visualization capabilities of PCA and t-SNE.
Discuss the trade-offs and scenarios where one technique may outperform the other.

## Project Report:
A comprehensive project report detailing the steps taken, results obtained, and your analysis.
Include visualizations, comparisons, and insights gained from the dimensionality reduction techniques.